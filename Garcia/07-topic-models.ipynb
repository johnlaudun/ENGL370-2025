{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e4941f9d-c5a1-46fb-bcde-f395d41cdfcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LDA Topics:\n",
      "Topic 1:\n",
      "don int just know like day\n",
      "\n",
      "\n",
      "Topic 2:\n",
      "int don just know like looks\n",
      "\n",
      "\n",
      "Topic 3:\n",
      "just don int like know day\n",
      "\n",
      "\n",
      "Topic 4:\n",
      "just int don like know day\n",
      "\n",
      "\n",
      "NMF Topics:\n",
      "Topic 1:\n",
      "int just continued don like know\n",
      "\n",
      "\n",
      "Topic 2:\n",
      "ted elaine healy lori john paul\n",
      "\n",
      "\n",
      "Topic 3:\n",
      "adam emma jacob seth nick eve\n",
      "\n",
      "\n",
      "Topic 4:\n",
      "rick fred elijah melanie debbie maggie\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import glob\n",
    "from sklearn.decomposition import LatentDirichletAllocation, NMF\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# Corpus is pointing to my comedy text\n",
    "path_to_corpus = r'C:\\Users\\alex\\code\\ENGL370-2025\\ENGL370-2025\\Garcia\\Data\\Comedy'\n",
    "\n",
    "# Read all my documents that is in my corpus\n",
    "documents = [open(file, 'r', encoding='utf-8').read() for file in glob.glob(os.path.join(path_to_corpus, '*.txt'))]\n",
    "\n",
    "# Give me the LDA Topics but not reading them\n",
    "vectorizer_lda = CountVectorizer(stop_words='english')\n",
    "dtm_lda = vectorizer_lda.fit_transform(documents)\n",
    "lda = LatentDirichletAllocation(n_components=4, random_state=42)\n",
    "lda.fit(dtm_lda)\n",
    "\n",
    "# Read me the lDA topics\n",
    "print(\"LDA Topics:\")\n",
    "for topic_idx, topic in enumerate(lda.components_):\n",
    "    print(f\"Topic {topic_idx + 1}:\")\n",
    "    print(\" \".join([vectorizer_lda.get_feature_names_out()[i] for i in topic.argsort()[:-6 - 1:-1]]))\n",
    "    print(\"\\n\")\n",
    "\n",
    "# Give me the NMF information\n",
    "vectorizer_nmf = TfidfVectorizer(stop_words='english')\n",
    "dtm_nmf = vectorizer_nmf.fit_transform(documents)\n",
    "nmf = NMF(n_components=4, random_state=42)\n",
    "nmf.fit(dtm_nmf)\n",
    "\n",
    "# Display NMF topics\n",
    "print(\"NMF Topics:\")\n",
    "for topic_idx, topic in enumerate(nmf.components_):\n",
    "    print(f\"Topic {topic_idx + 1}:\")\n",
    "    print(\" \".join([vectorizer_nmf.get_feature_names_out()[i] for i in topic.argsort()[:-6 - 1:-1]]))\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0589b381-1445-4557-a733-d49549b43ec0",
   "metadata": {},
   "source": [
    "First I loaded the data from my comedy library. Both in LDA and NMF topics.\n",
    "The LDA gave me broad topics that is hard to understand. LDA is good to work with my larger documents. Which is good for my comedies.\n",
    "It recongnizes words that act together. The nmf analyzes rare words and breaks the text down into smaller bits. I found that this work better and analyzed my comedies better even though the topic is still the same. It picked up more names than anything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4b6a2f-ea63-4c6a-8db6-e57d8b41615d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
