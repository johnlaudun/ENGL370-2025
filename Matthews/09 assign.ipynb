{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1e19bdc9-7461-4f53-992a-b2b0540f811c",
   "metadata": {},
   "source": [
    "First step was to load all my romance files so I got all my .txt files in my folder in my folder. These are all the texts of the romance novels I used python pathlib to load them which is good for handling a bunch of files this was the easiest part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00df24c1-f55e-4822-9ca2-d4ba85bef9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16a9f88-32cc-43b0-a072-87d18092e631",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the current directory (since the notebook and .txt files are in the same folder)\n",
    "romance_folder = Path(\".\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c04eb99-f8b1-4507-86cf-fca69e937bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all .txt files in the folder\n",
    "romance_files = list(romance_folder.glob(\"*.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eab6b3b-b8f9-42ed-a1c9-a997f69e4757",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the content of all romance text files into a dictionary\n",
    "romance_texts = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e93b84-ddd0-4206-9578-3f8f6d789a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "for file_path in romance_files:\n",
    "    with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "        romance_texts[file_path.name] = f.read()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129584da-ef9e-46ed-ba04-f3eb2f88e40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print summary\n",
    "print(f\"Loaded {len(mystery(3).zip_texts)} files:\")\n",
    "for name in mystery_texts:\n",
    "    print(\"-\", name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f469d5ad-1d8d-4ced-a52e-3aefbb1d4c7c",
   "metadata": {},
   "source": [
    "LOloaded 112 files:\n",
    "8mm\n",
    "\n",
    "A Few Good Men\n",
    "\n",
    "Agnes of God\n",
    "\n",
    "Angels & Demons\n",
    "\n",
    "April Fools Day\n",
    "\n",
    "Avventura (The Adventure)\n",
    "\n",
    "Backdraft\n",
    "\n",
    "Basic\n",
    "\n",
    "Basic Instinct\n",
    "\n",
    "Big Lebowski (The)\n",
    "\n",
    "Black Dahlia (The)\n",
    "\n",
    "Black Swan\n",
    "\n",
    "Blue Velvet\n",
    "\n",
    "Bourne Identity (The)\n",
    "\n",
    "Bourne Ultimatum (The)\n",
    "\n",
    "Box (The)\n",
    "\n",
    "Brick\n",
    "\n",
    "Buried\n",
    "\n",
    "Case 39\n",
    "\n",
    "Cellular\n",
    "\n",
    "Changeling\n",
    "\n",
    "Charade\n",
    "\n",
    "Cherry Falls\n",
    "\n",
    "Chinatown\n",
    "\n",
    "Citizen Kane\n",
    "\n",
    "Color of Night\n",
    "\n",
    "Copycat\n",
    "\n",
    "Cube\n",
    "\n",
    "Curious Case of Benjamin Button (The)\n",
    "\n",
    "Dark City\n",
    "\n",
    "Deception\n",
    "\n",
    "Devil in a Blue Dress\n",
    "\n",
    "Eagle Eye\n",
    "\n",
    "Eastern Promises\n",
    "\n",
    "Friday the 13th\n",
    "\n",
    "Fugitive (The)\n",
    "\n",
    "Game (The)\n",
    "\n",
    "Get Low\n",
    "\n",
    "Girl with the Dragon Tattoo (The)\n",
    "\n",
    "Godfather Part III (The)\n",
    "\n",
    "Gothika\n",
    "\n",
    "Grudge (The)\n",
    "\n",
    "Hangover (The)\n",
    "\n",
    "Hanna\n",
    "\n",
    "Haunting (The)\n",
    "\n",
    "Hellraiser: Deader\n",
    "\n",
    "Imaginarium of Doctor Parnassus (The)\n",
    "\n",
    "Inception\n",
    "\n",
    "I Still Know What You Did Last Summer\n",
    "\n",
    "Jennifer Eight\n",
    "\n",
    "JFK\n",
    "\n",
    "King Kong\n",
    "\n",
    "Klute\n",
    "\n",
    "L.A. Confidential\n",
    "\n",
    "Limitless\n",
    "\n",
    "Lone Star\n",
    "\n",
    "Losers (The)\n",
    "\n",
    "Lost Highway\n",
    "\n",
    "Lost Horizon\n",
    "\n",
    "Manchurian Candidate (The)\n",
    "\n",
    "Manhattan Murder Mystery\n",
    "\n",
    "Meet Joe Black\n",
    "\n",
    "Memento\n",
    "\n",
    "Minority Report\n",
    "\n",
    "Mirrors\n",
    "\n",
    "Mission Impossible II\n",
    "\n",
    "Moon\n",
    "\n",
    "Mulholland Drive\n",
    "\n",
    "Nightmare on Elm Street (A)\n",
    "\n",
    "Nightmare on Elm Street: The Final Chapter\n",
    "\n",
    "Nines (The)\n",
    "\n",
    "Ninth Gate (The)\n",
    "\n",
    "Orphan\n",
    "\n",
    "Private Life of Sherlock Holmes (The)\n",
    "\n",
    "Prometheus\n",
    "\n",
    "Prom Night\n",
    "\n",
    "Prophecy (The)\n",
    "\n",
    "Rear Window\n",
    "\n",
    "Red Riding Hood\n",
    "\n",
    "Ringu\n",
    "\n",
    "S. Darko\n",
    "\n",
    "Saw\n",
    "\n",
    "Scream\n",
    "\n",
    "Scream 2\n",
    "\n",
    "Scream 3\n",
    "\n",
    "Se7en\n",
    "\n",
    "Sherlock Holmes\n",
    "\n",
    "Six Degrees of Separation\n",
    "\n",
    "Sixth Sense (The)\n",
    "\n",
    "Sleepy Hollow\n",
    "\n",
    "Source Code\n",
    "\n",
    "Stir of Echoes\n",
    "\n",
    "Strange Days\n",
    "\n",
    "Super 8\n",
    "\n",
    "Talented Mr. Ripley (The)\n",
    "\n",
    "Tall in the Saddle\n",
    "\n",
    "Thunderheart\n",
    "\n",
    "Tinker Tailor Soldier Spy\n",
    "\n",
    "Twin Peaks\n",
    "\n",
    "Unknown\n",
    "\n",
    "Usual Suspects (The)\n",
    "\n",
    "Vanilla Sky\n",
    "\n",
    "What Lies Beneath\n",
    "\n",
    "White Jazz\n",
    "\n",
    "Whiteout\n",
    "\n",
    "White Ribbon (The)\n",
    "\n",
    "Wild Things: Diamonds in the Rough"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe1efb9-04ee-4506-8a94-2fab36496bfb",
   "metadata": {},
   "source": [
    "had a lot of trouble for some reason with these steps individually bc it kept making me re run the cell in order but i eventually just put it al together in one cell which seemed to work for me finally so dont mind the loading romance files again so in #2 \"tokenize\" i broke down each novel into individal tokens using nltk. this turned the unstructured text into the data and things i need to work with then in #3, i removed all the proper nouns by taking the tokens from each fil and tagging the word with the part of speech and removed the proper nouns in #4, i took each list of the words and turned it back into one big string which helps w the tdif vectorizer bc it needs a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8926de8-cafa-40fe-92d1-f4971170d951",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALL-IN-ONE CELL TO RESET AND PREP FOR TOPIC MODELING\n",
    "\n",
    "# 1. Load romance texts\n",
    "from pathlib import Path\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4555975-9146-474e-add4-553106172afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "mystery_folder = Path(\".\")\n",
    "mystery_files = list(romance_folder.glob(\"*.txt\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045db55e-e6f1-4831-816b-57c8d3c433fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "mystery_texts = {}\n",
    "for file_path in mystery_files:\n",
    "    with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "        romance_texts[file_path.name] = f.read()v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b0c2328-05fa-4f59-9f46-38fe6371dbd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Tokenize\n",
    "tokenized_texts = {}\n",
    "for filename, text in mystery_texts.items():\n",
    "    tokens = word_tokenize(text)\n",
    "    tokenized_texts[filename] = tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e54c12-6663-4d13-bf0d-3165141d4f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Remove proper nouns\n",
    "cleaned_texts = {\n",
    "    filename: [\n",
    "        word for word, tag in pos_tag(tokens)\n",
    "        if tag not in ('NNP', 'NNPS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2482171c-cac2-46dc-b580-9dffb57e531f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Remove proper nouns\n",
    "cleaned_texts = {\n",
    "    filename: [\n",
    "        word for word, tag in pos_tag(tokens)\n",
    "        if tag not in ('NNP', 'NNPS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0cae720c-8bc5-4f38-bbdb-96f1b2bee5cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All set! Ready for TF-IDF and topic modeling.\n"
     ]
    }
   ],
   "source": [
    "print(\"All set! Ready for TF-IDF and topic modeling.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "161aaa35-2fb5-4a90-8ae9-e979063bcf53",
   "metadata": {},
   "source": [
    "before i could tokenzie and tag parts of speech i had to make sure i got the nltk lib so it ended up being good so that was good "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7669e8a9-0a36-4ed5-b35b-aad8c889f79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "152a3c3d-e960-4a79-8776-fd1a0fc74723",
   "metadata": {},
   "source": [
    "\n",
    "[nltk_data] Downloading package punkt to\n",
    "[nltk_data]     /Users/bellabossier/nltk_data...\n",
    "[nltk_data]   Package punkt is already up-to-date!\n",
    "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
    "[nltk_data]     /Users/bellabossier/nltk_data...\n",
    "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
    "[nltk_data]       date!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "905d1fb8-fea3-4550-94a7-161ab9679012",
   "metadata": {},
   "source": [
    "\n",
    "True\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "finally for the fun stuff my text was clean and ready to vectorize so i used .9 to ignore the words that appear in 90 percent of books and 5 to ignore words that appear in fewer than 5 books and removed basic words like the is stuff like that then created a DTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134edf09-aef1-4160-b1d5-a71cac172264",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 5: TF-IDF VECTORIZE\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    max_df=0.9,\n",
    "    min_df=5,\n",
    "    stop_words='english'\n",
    ")\n",
    "\n",
    "dtm = tfidf_vectorizer.fit_transform(joined_cleaned_texts.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeece11c-ff2a-4d57-b7de-a752df64d64f",
   "metadata": {},
   "source": [
    "\n",
    "i was pretty much checking here bc the code would not run so i used this to seee the first 100 characters and it gave me a lot of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2bdb13bb-cd66-4162-8291-5c4c10c4125c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenized_texts' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlist\u001b[39m(\u001b[43mtokenized_texts\u001b[49m\u001b[38;5;241m.\u001b[39mitems())[:\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(joined_cleaned_texts[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m8mm.txte.txt\u001b[39m\u001b[38;5;124m'\u001b[39m][:\u001b[38;5;241m100\u001b[39m])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tokenized_texts' is not defined"
     ]
    }
   ],
   "source": [
    "print(list(tokenized_texts.items())[:1])\n",
    "print(joined_cleaned_texts['8mm.txte.txt'][:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f1c77f-090f-4fbd-b525-c808f4e6253a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(joined_cleaned_texts['8mm.txte.txt'][:100])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26026d1d-c49c-4e5e-b002-42ec8ae1ba44",
   "metadata": {},
   "source": [
    "[('8mm.txt', ['\"8MM\",', 'by', 'Andrew', 'Kevin', 'Walker', 'eight', 'millimeter', 'written', 'by', 'Andrew', 'Kevin', 'Walker', '5/06/97', 'first', 'INT.', 'MIAMI', 'AIRPORT,', 'TERMINAL', '--', 'DAY', 'Amongst', 'the', 'weary', 'tourist', 'families', 'and', 'solitary', 'businessmen', 'sits', 'TOM', 'WELLES,', 'middle-aged,', 'hair', 'neat,', 'suit', 'crisp', 'and', 'gray.', \"He's\", 'eating', 'crackers', 'from', 'a', 'cellophane', 'package,', 'sipping', 'soda', 'from', 'a', 'paper', 'cup,', 'watching', 'an', 'ARRIVAL', 'GATE.', 'AT', 'THE', 'GATE', 'PASSENGERS', 'arrive:', 'the', 'paunchy,', 'graying', 'men', 'of', 'First', 'Class', 'leading', 'the', 'pack,', 'except', 'for', 'a', 'handsome', 'YOUNG', 'REPUBLICAN', 'poster', 'boy', 'hurrying', 'along.', 'ACROSS', 'THE', 'TERMINAL', 'Welles', 'gets', 'up', 'and', 'FOLLOWS...', 'EXT.', 'MIAMI', 'AIRPORT,', 'CURBSIDE', '--', 'DAY', 'Welles', 'comes', 'outside,', 'squinting', 'in', 'the', 'sun,', 'moving', 'down', 'the', 'sidewalk,', 'looking', 'back', 'over', 'his', 'shoulder...'])]  \n",
    "('afewgoodmen.txt', ['FADE', 'IN:', 'A', 'Marines', 'Hymn', 'plays', 'over', 'black', 'screen.', 'FADE', 'IN', 'to', 'a', 'PLAZA', 'at', 'the', 'U.S.', 'Naval', 'Base', 'in', 'Guantanamo', 'Bay,', 'Cuba.', 'There', 'is', 'a', 'PRESENTATION', 'OF', 'THE', 'COLORS.', 'HUNDREDS', 'OF', 'MARINES', 'STAND', 'AT', 'ATTENTION.', 'A', 'ceremony', 'is', 'taking', 'place.', 'The', 'camera', 'tracks', 'past', 'several', 'rows', 'of', 'Marines,', 'young', 'men', 'standing', 'ramrod', 'straight', 'and', 'serious.'])]  \n",
    "('agnesofgod.txt', ['FADE', 'IN:', 'A', 'CHAPEL.', 'As', 'a', 'WOMAN', 'prays,', 'her', 'voice', 'is', 'heard', 'in', 'voiceover.', 'WOMAN', '(V.O.)', 'Hail', 'Mary,', 'full', 'of', 'grace,', 'the', 'Lord', 'is', 'with', 'thee...', 'CUT', 'TO:', 'A', 'DARK', 'CORRIDOR.', 'A', 'young', 'nun', 'in', 'a', 'habit', 'walks', 'down', 'the', 'hallway,', 'her', 'footsteps', 'echoing.', 'She', 'pauses', 'at', 'a', 'door.', 'INT.', 'NUNNERY', 'BEDROOM', '--', 'NIGHT.', 'The', 'young', 'nun', 'enters', 'the', 'room,', 'carrying', 'a', 'basket.', 'She', 'looks', 'around,', 'then', 'sets', 'the', 'basket', 'on', 'a', 'table.', 'She', 'lifts', 'the', 'lid.', 'Inside', 'is', 'a', 'NEWBORN', 'BABY,', 'wrapped', 'in', 'a', 'white', 'cloth.'])]  \n",
    "('angelsdemons.txt', ['FADE', 'IN:', 'A', 'STARRY', 'NIGHT', 'SKY.', 'The', 'camera', 'pans', 'down', 'to', 'reveal', 'the', 'Vatican,', 'lit', 'up', 'in', 'all', 'its', 'glory.', 'CUT', 'TO:', 'INT.', 'VATICAN', 'LIBRARY', '--', 'NIGHT.', 'ROBERT', 'LANGDON,', 'a', 'Harvard', 'professor', 'of', 'religious', 'iconology', 'and', 'symbology,', 'stands', 'before', 'a', 'large', 'old', 'book,', 'studying', 'its', 'contents.', 'He', 'glances', 'up', 'at', 'a', 'nearby', 'painting', 'of', 'Galileo.', 'CUT', 'TO:', 'A', 'MYSTERIOUS', 'FIGURE', 'cloaked', 'in', 'darkness,', 'walking', 'through', 'the', 'corridors', 'of', 'the', 'Vatican.'])]  \n",
    "('aprilfoolsday.txt', ['FADE', 'IN:', 'EXT.', 'COUNTRYSIDE', '--', 'DAY.', 'A', 'car', 'drives', 'down', 'a', 'winding', 'road', 'through', 'a', 'picturesque', 'landscape.', 'Inside,', 'a', 'group', 'of', 'YOUNG', 'ADULTS', 'are', 'laughing', 'and', 'talking.', 'CUT', 'TO:', 'INT.', 'CAR', '--', 'DAY.', 'Muffy,', 'a', 'young', 'woman', 'with', 'a', 'mischievous', 'glint', 'in', 'her', 'eye,', 'turns', 'to', 'her', 'friends.', 'MUFFY', 'I', 'hope', 'you', 'all', 'are', 'ready', 'for', 'the', 'best', 'April', 'Fools', 'weekend', 'ever!'])]  \n",
    "...\n",
    "('scream.txt', ['FADE', 'IN:', 'EXT.', 'SUBURBAN', 'HOUSE', '--', 'NIGHT.', 'A', 'young', 'woman,', 'CASEY,', 'is', 'alone', 'at', 'home.', 'The', 'phone', 'rings.', 'She', 'answers.', 'VOICE', '(V.O.)', 'Do', 'you', 'like', 'scary', 'movies?', 'CASEY', '(smiling)', 'Yeah,', 'I', 'guess.', 'VOICE', '(V.O.)', 'What’s', 'your', 'favorite', 'scary', 'movie?', 'CASEY', '(laughs)', 'I', 'don’t', 'know...', 'Halloween?', 'VOICE', '(V.O.)', 'That’s', 'a', 'classic.'])]  \n",
    "...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2851178-36fb-45f0-99f8-6fcd937d3083",
   "metadata": {},
   "source": [
    "another preview just to see if it looked good and making sure proper nouns were actually removed and readable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96dca605-0331-4116-b1b6-939769ae1880",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Sample joined text:\", list(joined_cleaned_texts.values())[0][:300])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582819c8-e7d8-4595-b927-87100d5d4193",
   "metadata": {},
   "source": [
    "Sample joined text: \"8MM\", by Andrew Kevin Walker eight millimeter written by Andrew Kevin Walker 5/06/97 first INT. MIAMI AIRPORT, TERMINAL -- DAY Amongst the weary tourist families and solitary businessmen sits TOM WELLES, middle-aged, hair neat, suit crisp and gray. He's eating crackers from a cellophane package, si\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d67c4b-5065-4a1c-91a3-6b9f31ac1d22",
   "metadata": {},
   "source": [
    "i asked nmf to find 10 diff topics in my collection but can tweak the numbers if i wanted to each row is one book and each column is the topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79fd6575-b33d-4a0e-8190-7ed258d5b7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "\n",
    "nmf_model = NMF(n_components=10, random_state=42)\n",
    "nmf_topics = nmf_model.fit_transform(dtm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8b49ff-57dd-4425-929d-f85b1088ad21",
   "metadata": {},
   "source": [
    "got the topics so now i get to see what they actually are feature names grabs all the words for each topic its sorted by importance and i got the top 10 then i print so i can see what each topic is about"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084753ce-e08c-4e66-99f3-abbca4f7d35b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 7: TOP WORDS PER TOPIC\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "for topic_idx, topic in enumerate(nmf_model.components_):\n",
    "    top_words = [feature_names[i] for i in topic.argsort()[-10:]]\n",
    "    print(f\"\\nTOPIC #{topic_idx + 1}:\\n\", top_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f74880cc-6312-4d4e-88cd-b71fac6a12f3",
   "metadata": {},
   "source": [
    "TOPIC#1 ['adam', 'cable', 'apartment', 'tom', 'tony', 'continued', 'ext', 'cont', 'martin', 'int']\n",
    "\n",
    "TOPIC#2 ['casey', 'billy', 'randy', 'stu', 'tatum', 'derek', 'sid', 'dewey', 'gale', 'sidney']\n",
    "\n",
    "TOPIC#3['wade', 'santiago', 'hollis', 'chris', 'ross', 'otis', 'dawson', 'pilar', 'corey', 'sam']\n",
    "\n",
    "TOPIC#4 ['mrs', 'baker', 'victoria', 'lestrade', 'hudson', 'mycroft', 'continued', 'irene', 'watson', 'holmes']\n",
    "\n",
    "TOPIC# 5 ['Thub', 'abbott', 'wombosi', 'elena', 'int', 'nicky', 'conklin', 'ross', 'marie', 'bourne']\n",
    "\n",
    "TOPIC# 6 ['dude', 'rasta', 'brian', 'jones', 'crs', 'collins', 'ybarra', 'walter', 'david', 'christine']\n",
    "\n",
    "TOPIC #7 ['coffin', 'alan', 'elizabeth', 'tess', 'larry', 'carol', 'dan', 'rick', 'geoffrey', 'paul']\n",
    "\n",
    "TOPIC #8['freddy', 'pete', 'jack', 'drew', 'cary', 'charles', 'susan', 'karen', 'alice', 'joe']\n",
    "\n",
    "TOPIC#9['donna', 'kane', 'brain', 'lucy', 'tug', 'leland', 'brad', 'pin', 'emily', 'laura']\n",
    "\n",
    "TOPIC #10 ['larry', 'welles', 'helen', 'benjamin', 'daisy', 'continued', 'nikolai', 'ben', 'anna', 'amy']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61aa43f7-d28e-439f-b197-d0719f20e858",
   "metadata": {},
   "source": [
    "trying lda for another look just comparing the nmd to lda so i did the same thing pretty much w 10 topics and used to same dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a88930-2784-4b49-a551-0066d8543caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "lda_model = LatentDirichletAllocation(n_components=10, random_state=42)\n",
    "lda_topics = lda_model.fit_transform(dtm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1583cb70-e96a-4c8c-a898-a801d1d45259",
   "metadata": {},
   "source": [
    "my comparision: nmf is cleaner and more distinct and just easier honestly lda is more repetitive and weird and just not useful in this case so i will pick nmf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f12542-9c42-40ee-b0c5-1f1e6cc4a268",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
