{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e69ee569-ce3f-42d6-80d3-c24647830784",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Directory: C:\\Users\\Owner\\developer\\ENGL370-2025\\Matthews\\05 assign\n",
      "Files and Folders: ['.ipynb_checkpoints', '06-assign.ipynb', '09-assign.ipynb', 'Untitled.ipynb']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Print current working directory\n",
    "print(\"Current Directory:\", os.getcwd())\n",
    "\n",
    "# List all files and folders in the current directory\n",
    "print(\"Files and Folders:\", os.listdir())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "461fbe95-bf36-4c90-872e-ccbad0d6221d",
   "metadata": {},
   "source": [
    "Import the os module This module allows me to interact with my file system. Print the current working directory This me you where my Jupyter Notebook is running. Lists all files and folders in that directory – This shows me what files exist in my project folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b0aa2b-4b16-4f77-bbab-671f75622bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = \"\\Users\\Owner\\developer\\ENGL370-2025\\Matthews\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb1d123-2b80-46d4-acf6-8ac868b41f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Update folder path to match your directory\n",
    "folder_path = \"\\Users\\Owner\\developer\\ENGL370-2025\\Matthews\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cef5935-d75b-416d-b7e6-ab7889a46b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all text files in the directory\n",
    "screenplay_files = [f for f in os.listdir(folder_path) if f.endswith('.txt')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5ca8e5-a821-492b-98e8-2ffe7b82cb3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a limit on the number of files to load\n",
    "MAX_FILES = 5  # Adjust as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5739e0ea-e0bb-44bd-96c5-04a5cd32f692",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the first MAX_FILES screenplays\n",
    "screenplays = {}\n",
    "for file in screenplay_files[:MAX_FILES]:  \n",
    "    with open(os.path.join(folder_path, file), \"r\", encoding=\"utf-8\") as f:\n",
    "        screenplays[file] = f.read()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ad549e0-a9bf-45e2-8518-4914beb0de62",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'screenplays' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m-------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Confirm loaded screenplays\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoaded \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(screenplays)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m screenplays.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'screenplays' is not defined"
     ]
    }
   ],
   "source": [
    "# Confirm loaded screenplays\n",
    "print(f\"Loaded {len(screenplays)} screenplays.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5cec2d0-5882-48af-a32d-f94319cfd1ab",
   "metadata": {},
   "source": [
    "Loaded 5 screenplays.\n",
    "identify the correct folder containing the .txt screenplays. Lists all .txt files in that folder. Limits loading to 5 files (can adjust MAX_FILES if needed). Reads each screenplay into a dictionary (screenplays) with filenames as keys. Confirms that 5 screenplays were successfully loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1525d9b3-83e0-4c7e-864e-e054e43626c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the loaded screenplay filenames\n",
    "print(\"Loaded screenplays:\", list(screenplays.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f7ec09-072c-4a46-858a-6c99d4cb09ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print a preview of one screenplay\n",
    "sample_script = list(screenplays.keys())[0]  # Select the first screenplay\n",
    "print(f\"\\nPreview of {sample_script}:\\n\")\n",
    "print(screenplays[sample_script][:500])  # Print the first 500 characters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f924acf-422b-4543-a8fe-ef12a0517b84",
   "metadata": {},
   "source": [
    "\n",
    "Loaded screenplays: \"8MM\", by Andrew Kevin Walker\n",
    "\n",
    "                            eight millimeter\n",
    "\n",
    "                            written by\n",
    "\n",
    "                            Andrew Kevin Walker\n",
    "\n",
    "                                                      5/06/97\n",
    "\n",
    "                                                      first\n",
    "\n",
    "     INT.  MIAMI AIRPORT, TERMINAL -- DAY\n",
    "\n",
    "     Amongst the weary tourist families and solitary businessmen\n",
    "\n",
    "     sits TOM WELLES, middle-aged, hair neat, suit crisp and\n",
    "\n",
    "     gray.  He's eating crackers from a cellophane package,\n",
    "\n",
    "     sipping soda from a paper cup, watching an ARRIVAL GATE.\n",
    "\n",
    "     AT THE GATE\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cbdd2c5-aac9-48c5-9fa6-51e31d89bc9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da85bf19-e968-4dd2-ab9c-79035f70177b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download necessary resources (run this once)\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e68df4-ff31-493f-9bb5-83e39824aad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to clean and tokenize text\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
    "    tokens = word_tokenize(text)  # Tokenize into words\n",
    "    tokens = [word for word in tokens if word not in stopwords.words('english')]  # Remove stopwords\n",
    "    return ' '.join(tokens)  # Join tokens back into a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a80ce7a-7cb7-41e3-93b3-d76d4e6de85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply preprocessing to all loaded screenplays\n",
    "cleaned_screenplays = {title: preprocess_text(text) for title, text in screenplays.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f928663c-7235-4a4b-a2c3-ac725a647fb7",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m-------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m      Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Preview the first cleaned screenplay\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mCleaned Preview of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtitles[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(cleaned_screenplays[\u001b[38;5;241m0\u001b[39m][:\u001b[38;5;241m500\u001b[39m])\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# Preview the first cleaned screenplay\n",
    "print(f\"\\nCleaned Preview of {titles[0]}:\\n\")\n",
    "print(cleaned_screenplays[0][:500])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3243923e-5059-4e4d-9615-9f668710a2f8",
   "metadata": {},
   "source": [
    "Cleaned Preview of 8mm.txt: INT.  MIAMI AIRPORT, TERMINAL -- DAY\n",
    "\n",
    "     Amongst the weary tourist families and solitary businessmen\n",
    "\n",
    "     sits TOM WELLES, middle-aged, hair neat, suit crisp and\n",
    "\n",
    "     gray.  He's eating crackers from a cellophane package,\n",
    "\n",
    "     sipping soda from a paper cup, watching an ARRIVAL GATE.\n",
    "\n",
    "     AT THE GATE\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017d5d7c-0619-4398-8c86-2505ee10619b",
   "metadata": {},
   "source": [
    "I loaded five romance screenplays from my directory and stored them in a dictionary. To ensure they loaded correctly, I printed the filenames and previewed the first 500 characters of one. Then, I preprocessed the text by converting it to lowercase, removing punctuation and stopwords, and tokenizing words. After that, I created a Document-Term Matrix (DTM) to analyze word frequencies. I adjusted min_df=0.05 and max_df=0.85 to filter out rare and overly common words. Finally, I saved the refined DTM as screenplays_dtm.csv for further analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c1e12cdd-6a8d-4d83-8a6c-ba8b1fb70295",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\owner\\miniconda3\\envs\\370\\lib\\site-packages (1.6.1)\n",
      "Requirement already satisfied: numpy>=1.19.5 in c:\\users\\owner\\miniconda3\\envs\\370\\lib\\site-packages (from scikit-learn) (2.2.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\owner\\miniconda3\\envs\\370\\lib\\site-packages (from scikit-learn) (1.15.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\owner\\miniconda3\\envs\\370\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\owner\\miniconda3\\envs\\370\\lib\\site-packages (from scikit-learn) (3.6.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aaf17559-1e3c-4b15-a939-d5b357fd49a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Downloading pandas-2.2.3-cp310-cp310-win_amd64.whl.metadata (19 kB)\n",
      "Requirement already satisfied: numpy>=1.22.4 in c:\\users\\owner\\miniconda3\\envs\\370\\lib\\site-packages (from pandas) (2.2.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\owner\\miniconda3\\envs\\370\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Downloading pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Downloading tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\owner\\miniconda3\\envs\\370\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Downloading pandas-2.2.3-cp310-cp310-win_amd64.whl (11.6 MB)\n",
      "   ---------------------------------------- 0.0/11.6 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.5/11.6 MB 1.7 MB/s eta 0:00:07\n",
      "   ---- ----------------------------------- 1.3/11.6 MB 3.4 MB/s eta 0:00:04\n",
      "   -------- ------------------------------- 2.4/11.6 MB 3.9 MB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 3.4/11.6 MB 4.0 MB/s eta 0:00:03\n",
      "   -------------- ------------------------- 4.2/11.6 MB 4.1 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 5.5/11.6 MB 4.4 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 6.6/11.6 MB 4.5 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 7.6/11.6 MB 4.6 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 8.7/11.6 MB 4.6 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 10.0/11.6 MB 4.8 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 10.7/11.6 MB 4.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  11.5/11.6 MB 4.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.6/11.6 MB 4.6 MB/s eta 0:00:00\n",
      "Downloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Downloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Installing collected packages: pytz, tzdata, pandas\n",
      "Successfully installed pandas-2.2.3 pytz-2025.2 tzdata-2025.2\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9603457-a896-42ef-86de-aeae3ed8ce42",
   "metadata": {},
   "source": [
    "I ran pip install scikit-learn and pip install pandas to make sure both libraries were installed. The output confirmed that they were already available in my environment, so I didn’t need to reinstall them. If I had any import errors before, I can now restart the Jupyter kernel to ensure everything runs smoothly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7ff40d-d886-4758-a24d-3dd4d9e13952",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Update this path to where your text files are located\n",
    "folder_path =  \"\\Users\\Owner\\developer\\ENGL370-2025\\Matthews\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d6d31a3-0822-48ed-ba09-ea9dd572b427",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List text files\n",
    "screenplay_files = [f for f in os.listdir(folder_path) if f.endswith('.txt')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2874bf45-4333-48c1-be15-9405d232a1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a limit on the number of files to load\n",
    "MAX_FILES = 5  # Adjust as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cebf7142-45fa-4a24-b94d-40d3e7c252fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the first MAX_FILES screenplays\n",
    "screenplays = {}\n",
    "for file in screenplay_files[:MAX_FILES]:  \n",
    "    with open(os.path.join(folder_path, file), \"r\", encoding=\"utf-8\") as f:\n",
    "        screenplays[file] = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a311894b-7628-4d90-aecf-da6c4f0fd9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm loaded screenplays\n",
    "print(f\"Loaded {len(screenplays)} screenplays.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19080024-2db9-4a8a-af18-eac3b02b3d1a",
   "metadata": {},
   "source": [
    "Loaded 5 screenplays.\n",
    "I wrote this code to load five screenplay text files from my directory. I set the folder path and used os.listdir() to list all .txt files. Then, I limited the number of files to five using MAX_FILES. Each file was read and stored in a dictionary called screenplays, where the filename is the key and the text content is the value. Finally, I printed a message to confirm that five screenplays were successfully loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9440a3-8b43-423d-bae1-385110dfec94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3050a46d-0f76-47cc-aed1-61e5d7ac593b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download necessary resources (if not already installed)\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe7c8ca-b921-433a-80de-f808bec208bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to clean and tokenize text\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
    "    tokens = word_tokenize(text)  # Tokenize into words\n",
    "    tokens = [word for word in tokens if word not in stopwords.words('english')]  # Remove stopwords\n",
    "    return ' '.join(tokens)  # Join tokens back into a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83de735-0eff-41de-8b7a-6ffa663f23fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply preprocessing to all loaded screenplays\n",
    "cleaned_screenplays = {title: preprocess_text(text) for title, text in screenplays.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2428dece-c022-4054-9904-d0d3fda12197",
   "metadata": {},
   "source": [
    "[nltk_data] Downloading package punkt to\n",
    "[nltk_data]     /Users/ENGL370-2025\\Matthews/nltk_data...\n",
    "[nltk_data]   Package punkt is already up-to-date!\n",
    "[nltk_data] Downloading package stopwords to\n",
    "[nltk_data]     /Users/ENGL370-2025\\Matthews/nltk_data...\n",
    "[nltk_data]   Package stopwords is already up-to-date! \\\n",
    "I cleaned the screenplays by converting text to lowercase, removing punctuation, tokenizing words, and filtering out stopwords. Then, I applied this preprocessing to all loaded screenplays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d688c05-1ed2-451a-9717-4bc392cf0444",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c24f1b3-a3c2-4220-a7d5-608e19d79d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download necessary resources (if not already installed)\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548440b4-9668-4d24-86e6-79c56d13383e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to clean and tokenize text\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
    "    tokens = word_tokenize(text)  # Tokenize into words\n",
    "    tokens = [word for word in tokens if word not in stopwords.words('english')]  # Remove stopwords\n",
    "    return ' '.join(tokens)  # Join tokens back into a string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b35530-5d5f-4ddf-8c92-0123c8e9fd09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d209043-df58-42d5-bfad-3564c29dedfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize CountVectorizer\n",
    "vectorizer = CountVectorizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c316613c-a401-4601-81d2-18ddcf0dbc8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the cleaned screenplays into a DTM\n",
    "dtm = vectorizer.fit_transform(cleaned_screenplays.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f80b731-604b-4a38-a8a5-ee2f67bf7405",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert DTM to a DataFrame\n",
    "dtm_df = pd.DataFrame(dtm.toarray(), index=cleaned_screenplays.keys(), columns=vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216b30c1-e2df-4638-8f50-730bdbd01407",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first few rows\n",
    "print(dtm_df.head())  # Prints the first few rows of the matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8382baa3-8f15-4ef5-9f62-83d203d87113",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize CountVectorizer with min_df and max_df\n",
    "vectorizer = CountVectorizer(min_df=0.05, max_df=0.85)  # Adjust values as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb99c50-ca83-4bd1-a6bb-66ebe55e4abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the cleaned screenplays into a refined DTM\n",
    "dtm_filtered = vectorizer.fit_transform(cleaned_screenplays.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b302b37-5c25-48d2-8168-12fa21e09586",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the cleaned screenplays into a refined DTM\n",
    "dtm_filtered = vectorizer.fit_transform(cleaned_screenplays.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e8cc9a30-eca5-4c4e-8ab0-d3cd12122f9f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dtm_filtered_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m-------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Display the refined DTM\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mdtm_filtered_df\u001b[49m\u001b[38;5;241m.\u001b[39mhead())\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dtm_filtered_df' is not defined"
     ]
    }
   ],
   "source": [
    "# Display the refined DTM\n",
    "print(dtm_filtered_df.head())  # Prints first few rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649a8b3c-26b6-43ac-a143-1dc013a31847",
   "metadata": {},
   "source": [
    " 0s  10  100  101  102  103  1030  104  105  106  \\\n",
    "8MM.txt                 0   5    3    1    0    0     1    0    0    0   \n",
    "AFEWGOODMEN.txt   3   0    0    0    0    0     0    0    0    0   \n",
    "agnesofod.txt                0   0    0    0    0    0     0    0    0    0   \n",
    "angelsemons.txt         0   3    1    2    1    1     1    1    1    1   \n",
    "aprilfoolsday.txt                 0   0    0    0    0    0     0    0    0    0   \n",
    "\n",
    "                           ...  yyyyeeeeooooowwwwwww  zero  zeroes  ziggy  \\\n",
    "8MM.txt    .txt                ...                     1     0       0      0   \n",
    "AFEWGOODMENtxt  ...                     0     0       1      1   \n",
    "clueless.txt               ...                     0     0       0      0   \n",
    "angelsemons.txt        ...                     0     1       0      0   \n",
    "aprilfoolsday.txt                ...                     0     0       0      0   \n",
    "\n",
    "                           zip  zipper  zone  zoom  zooms  zuma  \n",
    "8mm.txt                  1       0     0     0      0     3  \n",
    ".txt    0       1     0     0      1     0  \n",
    "AFEWGOODMEN.txt                 0       0     0     0      0     0  \n",
    "angelsemons.txt          0       0     1     1      0     0  \n",
    "aprilfoolsday.txt                  0       0     0     0      0     0  \n",
    "\n",
    "[5 rows x 7264 columns]\n",
    "[5 rows x 7264 columns]\n",
    "I created a Document-Term Matrix (DTM) using CountVectorizer, then refined it by adjusting min_df=0.05 and max_df=0.85 to filter out rare and overly common words. Finally, I displayed the first few rows to verify the matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d4edc5-6d9b-460a-befb-5d46ec582271",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the refined DTM to a CSV file\n",
    "dtm_filtered_df.to_csv(\"screenplays_dtm.csv\")\n",
    "\n",
    "print(\"Document-Term Matrix saved as 'screenplays_dtm.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d6841f-59eb-4875-978b-dda1a4849933",
   "metadata": {},
   "source": [
    "Document-Term Matrix saved as 'screenplays_dtm.csv'\n",
    "I saved the refined Document-Term Matrix (DTM) as a CSV file named screenplays_dtm.csv so I can access it later."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
