{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "857f7445-ce3c-451a-856a-c6a917efb8b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Directory: /Users/bellabossier/code/ENGL370-2025/Bossier\n",
      "Files and Folders: ['05-bella.ipynb', '.DS_Store', '03-bella.ipynb', '17again.txt', '06-bella.ipynb', 'ReadMe.md', '10thingsihateaboutyou.txt', '01-bella.ipynb', 'clueless.txt', '04-bella.ipynb', '.ipynb_checkpoints', 'screenplays_dtm.csv', '500daysofsummer.txt', 'Macbeth.txt', 'mygirl.txt', '02-bella.ipynb']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Print current working directory\n",
    "print(\"Current Directory:\", os.getcwd())\n",
    "\n",
    "# List all files and folders in the current directory\n",
    "print(\"Files and Folders:\", os.listdir())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3038422-37c7-4dc4-8d50-e05f056b41e0",
   "metadata": {},
   "source": [
    "Import the os module\n",
    "This module allows me to interact with my file system.\n",
    "Print the current working directory This me you where my Jupyter Notebook is running.\n",
    "Lists all files and folders in that directory â€“ This shows me what files exist in my project folder.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bce60005-4820-45fa-addc-3fc70b558681",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = \"/Users/bellabossier/code/ENGL370-2025/Bossier\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d07ad53d-ed8c-425f-9a70-119efee8d671",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 5 screenplays.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Update folder path to match your directory\n",
    "folder_path = \"/Users/bellabossier/code/ENGL370-2025/Bossier\"\n",
    "\n",
    "# List all text files in the directory\n",
    "screenplay_files = [f for f in os.listdir(folder_path) if f.endswith('.txt')]\n",
    "\n",
    "# Set a limit on the number of files to load\n",
    "MAX_FILES = 5  # Adjust as needed\n",
    "\n",
    "# Read the first MAX_FILES screenplays\n",
    "screenplays = {}\n",
    "for file in screenplay_files[:MAX_FILES]:  \n",
    "    with open(os.path.join(folder_path, file), \"r\", encoding=\"utf-8\") as f:\n",
    "        screenplays[file] = f.read()\n",
    "\n",
    "# Confirm loaded screenplays\n",
    "print(f\"Loaded {len(screenplays)} screenplays.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e6f11f1-51aa-466c-ad4e-285076f7cac5",
   "metadata": {},
   "source": [
    "identify the correct folder containing the .txt screenplays.\n",
    "Lists all .txt files in that folder.\n",
    "Limits loading to 5 files  (can adjust MAX_FILES if needed).\n",
    "Reads each screenplay into a dictionary (screenplays) with filenames as keys.\n",
    "Confirms that 5 screenplays were successfully loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d138de10-eebd-431a-86ab-08d2a49f341e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded screenplays: ['17again.txt', '10thingsihateaboutyou.txt', 'clueless.txt', '500daysofsummer.txt', 'Macbeth.txt']\n",
      "\n",
      "Preview of 17again.txt:\n",
      "\n",
      "                                     17 AGAIN\n",
      "\n",
      "                                    Written by\n",
      "\n",
      "                                   Jason Filardi\n",
      "\n",
      "                                                         October 2007\n",
      "\n",
      "          EXT. FITCH SENIOR HIGH SCHOOL - DUSK\n",
      "\n",
      "          A few cars scatter the parking lot. WE hear GRUNTS followed\n",
      "\n",
      "          by the distinct sound of basketballs shredding net.\n",
      "\n",
      "          INT. FITCH SENIOR HIGH SCHOOL/GYM - CONTINUOUS\n",
      "\n",
      "          An empty gymnasium except for a sh\n"
     ]
    }
   ],
   "source": [
    "# Print the loaded screenplay filenames\n",
    "print(\"Loaded screenplays:\", list(screenplays.keys()))\n",
    "\n",
    "# Print a preview of one screenplay\n",
    "sample_script = list(screenplays.keys())[0]  # Select the first screenplay\n",
    "print(f\"\\nPreview of {sample_script}:\\n\")\n",
    "print(screenplays[sample_script][:500])  # Print the first 500 characters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0136297-6de2-4e26-a8c9-55c2a86d605e",
   "metadata": {},
   "source": [
    "Confirms 5 screenplays are loaded (17again.txt, 10thingsihateaboutyou.txt, clueless.txt, 500daysofsummer.txt, Macbeth.txt). \n",
    "Prints a preview of one screenplay (17again.txt), showing the first 500 characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "113e99c2-c6c9-412f-8290-5291fb5d426a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/bellabossier/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/bellabossier/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cleaned Preview of 17again.txt:\n",
      "\n",
      "17 written jason filardi october 2007 ext fitch senior high school dusk cars scatter parking lot hear grunts followed distinct sound basketballs shredding net int fitch senior high schoolgym continuous empty gymnasium except shirtless mike odonnell 17 mike stands feet beyond 3 point line grabs balls hopper rapidly shoots shoots shoots swishswishswish kids automatic mikes hair pompadour mullet la 21 jump street short shorts circa 1989 mustached curly haired coach harvey 40 enters coach harvey hey\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Download necessary resources (run this once)\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Define a function to clean and tokenize text\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
    "    tokens = word_tokenize(text)  # Tokenize into words\n",
    "    tokens = [word for word in tokens if word not in stopwords.words('english')]  # Remove stopwords\n",
    "    return ' '.join(tokens)  # Join tokens back into a string\n",
    "\n",
    "# Apply preprocessing to all loaded screenplays\n",
    "cleaned_screenplays = {title: preprocess_text(text) for title, text in screenplays.items()}\n",
    "\n",
    "# Print a preview of cleaned text\n",
    "sample_script = list(cleaned_screenplays.keys())[0]\n",
    "print(f\"\\nCleaned Preview of {sample_script}:\\n\")\n",
    "print(cleaned_screenplays[sample_script][:500])  # Show first 500 characters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944efbf6-f233-4e7d-85b0-1563479f5efd",
   "metadata": {},
   "source": [
    "I loaded five romance screenplays from my directory and stored them in a dictionary. To ensure they loaded correctly, I printed the filenames and previewed the first 500 characters of one. Then, I preprocessed the text by converting it to lowercase, removing punctuation and stopwords, and tokenizing words. After that, I created a Document-Term Matrix (DTM) to analyze word frequencies. I adjusted min_df=0.05 and max_df=0.85 to filter out rare and overly common words. Finally, I saved the refined DTM as screenplays_dtm.csv for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e065ad30-0206-4dd3-bd29-ee7669fccd18",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in /opt/anaconda3/envs/370/lib/python3.10/site-packages (1.6.1)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /opt/anaconda3/envs/370/lib/python3.10/site-packages (from scikit-learn) (2.2.3)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /opt/anaconda3/envs/370/lib/python3.10/site-packages (from scikit-learn) (1.15.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/anaconda3/envs/370/lib/python3.10/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/anaconda3/envs/370/lib/python3.10/site-packages (from scikit-learn) (3.6.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "287be0b7-c14c-4098-96f7-8279e6b27f5e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /opt/anaconda3/envs/370/lib/python3.10/site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy>=1.22.4 in /opt/anaconda3/envs/370/lib/python3.10/site-packages (from pandas) (2.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/envs/370/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/envs/370/lib/python3.10/site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/envs/370/lib/python3.10/site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/envs/370/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdce6f81-8ab2-4e3a-9f9d-0219abcb7265",
   "metadata": {},
   "source": [
    "I ran pip install scikit-learn and pip install pandas to make sure both libraries were installed. The output confirmed that they were already available in my environment, so I didnâ€™t need to reinstall them. If I had any import errors before, I can now restart the Jupyter kernel to ensure everything runs smoothly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f6c5032d-4871-4360-bbc7-57420b6b48a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 5 screenplays.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Update this path to where your text files are located\n",
    "folder_path = \"/Users/bellabossier/code/ENGL370-2025/Bossier\"\n",
    "\n",
    "# List text files\n",
    "screenplay_files = [f for f in os.listdir(folder_path) if f.endswith('.txt')]\n",
    "\n",
    "# Set a limit on the number of files to load\n",
    "MAX_FILES = 5  # Adjust as needed\n",
    "\n",
    "# Read the first MAX_FILES screenplays\n",
    "screenplays = {}\n",
    "for file in screenplay_files[:MAX_FILES]:  \n",
    "    with open(os.path.join(folder_path, file), \"r\", encoding=\"utf-8\") as f:\n",
    "        screenplays[file] = f.read()\n",
    "\n",
    "# Confirm loaded screenplays\n",
    "print(f\"Loaded {len(screenplays)} screenplays.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "032edcd9-ef92-4d95-aa75-8480f3eab565",
   "metadata": {},
   "source": [
    "I wrote this code to load five screenplay text files from my directory. I set the folder path and used os.listdir() to list all .txt files. Then, I limited the number of files to five using MAX_FILES. Each file was read and stored in a dictionary called screenplays, where the filename is the key and the text content is the value. Finally, I printed a message to confirm that five screenplays were successfully loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8b687c00-ccf9-4963-bb3a-9a3d59ca69fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/bellabossier/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/bellabossier/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Download necessary resources (if not already installed)\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Define a function to clean and tokenize text\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
    "    tokens = word_tokenize(text)  # Tokenize into words\n",
    "    tokens = [word for word in tokens if word not in stopwords.words('english')]  # Remove stopwords\n",
    "    return ' '.join(tokens)  # Join tokens back into a string\n",
    "\n",
    "# Apply preprocessing to all loaded screenplays\n",
    "cleaned_screenplays = {title: preprocess_text(text) for title, text in screenplays.items()}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb39d7ff-62b3-42c0-8632-4a6534f094f7",
   "metadata": {},
   "source": [
    "I cleaned the screenplays by converting text to lowercase, removing punctuation, tokenizing words, and filtering out stopwords. Then, I applied this preprocessing to all loaded screenplays.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b52190d7-2d5e-4e5a-85ed-0563645b0576",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/bellabossier/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/bellabossier/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cleaned Preview of 17again.txt:\n",
      "\n",
      "17 written jason filardi october 2007 ext fitch senior high school dusk cars scatter parking lot hear grunts followed distinct sound basketballs shredding net int fitch senior high schoolgym continuous empty gymnasium except shirtless mike odonnell 17 mike stands feet beyond 3 point line grabs balls hopper rapidly shoots shoots shoots swishswishswish kids automatic mikes hair pompadour mullet la 21 jump street short shorts circa 1989 mustached curly haired coach harvey 40 enters coach harvey hey\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Download necessary resources (if not already installed)\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Define a function to clean and tokenize text\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
    "    tokens = word_tokenize(text)  # Tokenize into words\n",
    "    tokens = [word for word in tokens if word not in stopwords.words('english')]  # Remove stopwords\n",
    "    return ' '.join(tokens)  # Join tokens back into a string\n",
    "\n",
    "# Apply preprocessing to all loaded screenplays\n",
    "cleaned_screenplays = {title: preprocess_text(text) for title, text in screenplays.items()}\n",
    "\n",
    "# Print a preview of cleaned text\n",
    "sample_script = list(cleaned_screenplays.keys())[0]\n",
    "print(f\"\\nCleaned Preview of {sample_script}:\\n\")\n",
    "print(cleaned_screenplays[sample_script][:500])  # Show first 500 characters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e3306c-84e9-483a-afdc-0b3761e23fa7",
   "metadata": {},
   "source": [
    "I cleaned and tokenized the screenplays, removed stopwords, and displayed a preview of the processed text.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96851f4-fb0f-4987-ae3b-aa7846689084",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "# Initialize CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Transform the cleaned screenplays into a DTM\n",
    "dtm = vectorizer.fit_transform(cleaned_screenplays.values())\n",
    "\n",
    "# Convert DTM to a DataFrame\n",
    "dtm_df = pd.DataFrame(dtm.toarray(), index=cleaned_screenplays.keys(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "# Display the first few rows\n",
    "print(dtm_df.head())  # Prints the first few rows of the matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3540215f-bbf4-4241-90f2-1496b17b280b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           0s  10  100  101  102  103  1030  104  105  106  \\\n",
      "17again.txt                 0   5    3    1    0    0     1    0    0    0   \n",
      "10thingsihateaboutyou.txt   3   0    0    0    0    0     0    0    0    0   \n",
      "clueless.txt                0   0    0    0    0    0     0    0    0    0   \n",
      "500daysofsummer.txt         0   3    1    2    1    1     1    1    1    1   \n",
      "Macbeth.txt                 0   0    0    0    0    0     0    0    0    0   \n",
      "\n",
      "                           ...  yyyyeeeeooooowwwwwww  zero  zeroes  ziggy  \\\n",
      "17again.txt                ...                     1     0       0      0   \n",
      "10thingsihateaboutyou.txt  ...                     0     0       1      1   \n",
      "clueless.txt               ...                     0     0       0      0   \n",
      "500daysofsummer.txt        ...                     0     1       0      0   \n",
      "Macbeth.txt                ...                     0     0       0      0   \n",
      "\n",
      "                           zip  zipper  zone  zoom  zooms  zuma  \n",
      "17again.txt                  1       0     0     0      0     3  \n",
      "10thingsihateaboutyou.txt    0       1     0     0      1     0  \n",
      "clueless.txt                 0       0     0     0      0     0  \n",
      "500daysofsummer.txt          0       0     1     1      0     0  \n",
      "Macbeth.txt                  0       0     0     0      0     0  \n",
      "\n",
      "[5 rows x 7264 columns]\n"
     ]
    }
   ],
   "source": [
    "# Initialize CountVectorizer with min_df and max_df\n",
    "vectorizer = CountVectorizer(min_df=0.05, max_df=0.85)  # Adjust values as needed\n",
    "\n",
    "# Transform the cleaned screenplays into a refined DTM\n",
    "dtm_filtered = vectorizer.fit_transform(cleaned_screenplays.values())\n",
    "\n",
    "# Convert DTM to a DataFrame\n",
    "dtm_filtered_df = pd.DataFrame(dtm_filtered.toarray(), index=cleaned_screenplays.keys(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "# Display the refined DTM\n",
    "print(dtm_filtered_df.head())  # Prints first few rows\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ccf4061-945a-4334-bc21-31577b5ea2ff",
   "metadata": {},
   "source": [
    "I created a Document-Term Matrix (DTM) using CountVectorizer, then refined it by adjusting min_df=0.05 and max_df=0.85 to filter out rare and overly common words. Finally, I displayed the first few rows to verify the matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "40e0d876-0779-4958-9328-3dee76a93f41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document-Term Matrix saved as 'screenplays_dtm.csv'\n"
     ]
    }
   ],
   "source": [
    "# Save the refined DTM to a CSV file\n",
    "dtm_filtered_df.to_csv(\"screenplays_dtm.csv\")\n",
    "\n",
    "print(\"Document-Term Matrix saved as 'screenplays_dtm.csv'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f9f5bc-196c-40e2-95f9-ee0ef5c47f98",
   "metadata": {},
   "source": [
    "I saved the refined Document-Term Matrix (DTM) as a CSV file named screenplays_dtm.csv so I can access it later.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12868f72-a3ea-4837-a3c8-bd091f7ca58b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:370]",
   "language": "python",
   "name": "conda-env-370-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
