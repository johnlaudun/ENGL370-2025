{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f6d905b8-594a-40d9-9e9d-fbb3553df850",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/beauxcreel/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/beauxcreel/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 5 screenplays.\n",
      "Loaded screenplays: ['aladdin.txt', 'princessbridethe.txt', 'findingnemo.txt', 'kungfupanda.txt', 'e.t..txt']\n",
      "\n",
      "Preview of aladdin.txt:\n",
      "\n",
      "ALADDIN:  THE COMPLETE SCRIPT\n",
      "\n",
      "COMPILED BY BEN SCRIPPS \n",
      "\n",
      "(Portions Copyright (c) 1992 The Walt Disney Company\n",
      "\n",
      "PEDDLER:    Oh I come from a land\n",
      "\n",
      "    From a faraway place\n",
      "\n",
      "    Where the caravan camels roam\n",
      "\n",
      "    Where they cut off your ear /Where it's flat and immense\n",
      "\n",
      "    If they don't like your face /And the heat is intense\n",
      "\n",
      "    It's barbaric, but hey--it's home!\n",
      "\n",
      "    When the wind's at your back\n",
      "\n",
      "    And the sun's from the west\n",
      "\n",
      "    And the sand in the glass is right\n",
      "\n",
      "    Come on down,\n",
      "\n",
      "    St\n",
      "\n",
      "Cleaned Preview of aladdin.txt:\n",
      "\n",
      "aladdin complete script compiled ben scripps portions copyright c 1992 walt disney company peddler oh come land faraway place caravan camels roam cut ear flat immense dont like face heat intense barbaric heyits home winds back suns west sand glass right come stop hop carpet fly another arabian night arabian nights like arabian days often hotter hot lot good ways arabian nights neath arabian moons fool guard could fall fall hard dunes ah salaam good evening worthy friend please please come closer\n",
      "                      10  100  101  102  103  104  105  106  106a  107  ...  \\\n",
      "aladdin.txt            0    0    0    0    0    0    0    0     0    0  ...   \n",
      "princessbridethe.txt   1    1    1    1    1    1    1    1     0    1  ...   \n",
      "findingnemo.txt        1    1    0    0    0    0    0    1     1    1  ...   \n",
      "kungfupanda.txt        1    0    0    0    0    0    0    0     0    0  ...   \n",
      "e.t..txt               0    0    0    0    0    0    0    0     0    0  ...   \n",
      "\n",
      "                      zero  zeroed  zings  zip  zips  zombie  zones  zoo  \\\n",
      "aladdin.txt              0       0      1    0     1       1      0    1   \n",
      "princessbridethe.txt     0       1      0    0     0       0      0    0   \n",
      "findingnemo.txt          0       0      0    0     0       0      4    1   \n",
      "kungfupanda.txt          4       0      0    0     0       0      0    0   \n",
      "e.t..txt                 0       0      0    1     2       0      0    0   \n",
      "\n",
      "                      zoom  zooms  \n",
      "aladdin.txt              4      8  \n",
      "princessbridethe.txt     0      1  \n",
      "findingnemo.txt          0      0  \n",
      "kungfupanda.txt          1      0  \n",
      "e.t..txt                 0      0  \n",
      "\n",
      "[5 rows x 7402 columns]\n",
      "Document-Term Matrix saved as 'screenplays_dtm.csv'.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nFinal Outcome:\\n--------------\\nThis script provides a foundation for further text analysis, such as:\\n- Word frequency studies to identify common and unique words in screenplays.\\n- Topic modeling to categorize scripts based on prevalent themes.\\n- Sentiment analysis to examine emotional tones in screenplays.\\n- Machine learning applications for automated text classification.\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\"\"\"\n",
    "This script processes text files containing screenplays and converts them into a structured Document-Term Matrix (DTM) for analysis.\n",
    "\n",
    "Steps:\n",
    "1. Load necessary NLTK resources for text processing.\n",
    "2. Retrieve and read screenplay files from a specified folder.\n",
    "3. Preprocess the text by:\n",
    "   - Converting to lowercase\n",
    "   - Removing punctuation\n",
    "   - Tokenizing words\n",
    "   - Removing common stopword\n",
    "4. Transform the cleaned text into a Document-Term Matrix (DTM) using CountVectorizer.\n",
    "5. Save the DTM as a CSV file for further analysis.\n",
    "\"\"\"\n",
    "\n",
    "# Ensure necessary NLTK resources are downloaded\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Define the path to the \"family.\" folder containing screenplay text files\n",
    "folder_path = \"/Users/beauxcreel/code/ENGL370-2025/Creel/Family.\"\n",
    "\n",
    "# Retrieve a list of all text files in the specified folder\n",
    "screenplay_files = [f for f in os.listdir(folder_path) if f.endswith('.txt')]\n",
    "\n",
    "# Limit the number of files to process to avoid excessive memory usage\n",
    "MAX_FILES = 5  # Adjust as needed for scalability\n",
    "screenplay_files = screenplay_files[:MAX_FILES]  # Ensure only 5 files are processed\n",
    "\n",
    "# Load the screenplays into a dictionary\n",
    "screenplays = {}\n",
    "for file in screenplay_files:\n",
    "    with open(os.path.join(folder_path, file), \"r\", encoding=\"utf-8\") as f:\n",
    "        screenplays[file] = f.read()\n",
    "\n",
    "# Display the number of loaded screenplays\n",
    "print(f\"Loaded {len(screenplays)} screenplays.\")\n",
    "print(\"Loaded screenplays:\", list(screenplays.keys()))\n",
    "\n",
    "# Preview the content of the first screenplay (first 500 characters)\n",
    "sample_script = list(screenplays.keys())[0]\n",
    "print(f\"\\nPreview of {sample_script}:\\n\")\n",
    "print(screenplays[sample_script][:500])\n",
    "\n",
    "# Function to clean and preprocess text\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Cleans and tokenizes text by:\n",
    "    - Converting to lowercase to standardize text\n",
    "    - Removing punctuation to reduce noise\n",
    "    - Tokenizing words using nltk's word_tokenize function\n",
    "    - Removing stopwords to focus on meaningful words\n",
    "    \n",
    "    This ensures that only relevant words are retained for analysis, improving the accuracy of text-based models.\n",
    "    \n",
    "    Returns:\n",
    "        str: The cleaned and processed text as a single string\n",
    "    \"\"\"\n",
    "    text = text.lower()  # Convert text to lowercase\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation using regex\n",
    "    tokens = word_tokenize(text)  # Tokenize text into words\n",
    "    tokens = [word for word in tokens if word not in stopwords.words('english')]  # Remove stopwords\n",
    "    return ' '.join(tokens)  # Join tokens back into a cleaned string\n",
    "\n",
    "# Apply text preprocessing to all screenplays\n",
    "cleaned_screenplays = {title: preprocess_text(text) for title, text in screenplays.items()}\n",
    "\n",
    "# Display a preview of cleaned text\n",
    "sample_script = list(cleaned_screenplays.keys())[0]\n",
    "print(f\"\\nCleaned Preview of {sample_script}:\\n\")\n",
    "print(cleaned_screenplays[sample_script][:500])\n",
    "\n",
    "# Convert cleaned text into a Document-Term Matrix (DTM)\n",
    "vectorizer = CountVectorizer()\n",
    "dtm = vectorizer.fit_transform(cleaned_screenplays.values())\n",
    "\n",
    "# Create a DataFrame for easier visualization\n",
    "dtm_df = pd.DataFrame(dtm.toarray(), index=cleaned_screenplays.keys(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "# Display the first few rows of the DTM to examine word frequency across scripts\n",
    "print(dtm_df.head())\n",
    "\n",
    "# Save the Document-Term Matrix as a CSV file for future analysis\n",
    "dtm_df.to_csv(\"screenplays_dtm.csv\")\n",
    "print(\"Document-Term Matrix saved as 'screenplays_dtm.csv'.\")\n",
    "\n",
    "\"\"\"\n",
    "Final Outcome:\n",
    "--------------\n",
    "This script provides a foundation for further text analysis, such as:\n",
    "- Word frequency studies to identify common and unique words in screenplays.\n",
    "- Topic modeling to categorize scripts based on prevalent themes.\n",
    "- Sentiment analysis to examine emotional tones in screenplays.\n",
    "- Machine learning applications for automated text classification.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2db78a3-3296-4a41-86ff-b6ca4bb40d41",
   "metadata": {},
   "source": [
    "Cell 1:Import Libraries and Setup NLTK \n",
    "In this cell, we import the required libraries. We use:\n",
    "\n",
    "os for interacting with the file system (loading screenplays).\n",
    "\n",
    "re for regular expressions, which help clean and preprocess the text.\n",
    "\n",
    "nltk to process natural language, including downloading tokenizers and stopwords.\n",
    "\n",
    "pandas for organizing the data into a DataFrame.\n",
    "\n",
    "CountVectorizer from sklearn to convert text into a Document-Term Matrix (DTM) for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9f55855-7cd7-4a3e-b2f5-6ff01a7f0251",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/beauxcreel/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/beauxcreel/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Setup: Ensure NLTK knows where to download data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fdacc62-6086-4cd4-9e2b-8ac288a403d6",
   "metadata": {},
   "source": [
    "Cell 2:Load Screenplay Files\n",
    "This cell loads the screenplay text files from the specified folder. We limit the number of files processed to avoid running out of memory, particularly when working with large text files. After loading the screenplays, we print how many files were successfully loaded and display a preview of the first screenplay (first 500 characters) to check its content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8476399d-c39c-4f03-91ef-590f1a8b410c",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/beauxcreel/code/ENGL370-2025/Creel/Family.'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m folder_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/Users/beauxcreel/code/ENGL370-2025/Creel/Family.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Retrieve a list of all text files in the specified folder\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m screenplay_files \u001b[38;5;241m=\u001b[39m [f \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfolder_path\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m f\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.txt\u001b[39m\u001b[38;5;124m'\u001b[39m)]\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Limit the number of files to process to avoid excessive memory usage\u001b[39;00m\n\u001b[1;32m      8\u001b[0m MAX_FILES \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m  \u001b[38;5;66;03m# Adjust as needed for scalability\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/beauxcreel/code/ENGL370-2025/Creel/Family.'"
     ]
    }
   ],
   "source": [
    "# Define the path to the \"family.\" folder containing screenplay text files\n",
    "folder_path = \"/Users/beauxcreel/code/ENGL370-2025/Creel/Family.\"\n",
    "\n",
    "# Retrieve a list of all text files in the specified folder\n",
    "screenplay_files = [f for f in os.listdir(folder_path) if f.endswith('.txt')]\n",
    "\n",
    "# Limit the number of files to process to avoid excessive memory usage\n",
    "MAX_FILES = 5  # Adjust as needed for scalability\n",
    "screenplay_files = screenplay_files[:MAX_FILES]  # Ensure only 5 files are processed\n",
    "\n",
    "# Load the screenplays into a dictionary\n",
    "screenplays = {}\n",
    "for file in screenplay_files:\n",
    "    with open(os.path.join(folder_path, file), \"r\", encoding=\"utf-8\") as f:\n",
    "        screenplays[file] = f.read()\n",
    "\n",
    "# Display the number of loaded screenplays\n",
    "print(f\"Loaded {len(screenplays)} screenplays.\")\n",
    "print(\"Loaded screenplays:\", list(screenplays.keys()))\n",
    "\n",
    "# Preview the content of the first screenplay (first 500 characters)\n",
    "sample_script = list(screenplays.keys())[0]\n",
    "print(f\"\\nPreview of {sample_script}:\\n\")\n",
    "print(screenplays[sample_script][:500])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f5056ab-4839-42c7-9965-465fb9dcbb12",
   "metadata": {},
   "source": [
    "Cell 3:Preprocess the Text\n",
    "This cell defines the preprocess_text function, which performs several text cleaning tasks:\n",
    "\n",
    "Converts the text to lowercase to standardize it.\n",
    "\n",
    "Removes punctuation using a regular expression.\n",
    "\n",
    "Tokenizes the text into individual words using NLTK's word_tokenize.\n",
    "\n",
    "Removes stopwords (common words like \"the\", \"and\", etc.) that don't provide significant meaning for analysis.\n",
    "\n",
    "The function then returns the cleaned and tokenized text. After defining the function, we apply it to all the screenplays to prepare them for analysis. We also preview the cleaned text of the first screenplay to verify the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad9147dd-eb4f-4398-b203-41de02c9fd3c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'screenplays' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 22\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(tokens)  \u001b[38;5;66;03m# Join tokens back into a cleaned string\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Apply text preprocessing to all screenplays\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m cleaned_screenplays \u001b[38;5;241m=\u001b[39m {title: preprocess_text(text) \u001b[38;5;28;01mfor\u001b[39;00m title, text \u001b[38;5;129;01min\u001b[39;00m \u001b[43mscreenplays\u001b[49m\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Display a preview of cleaned text\u001b[39;00m\n\u001b[1;32m     25\u001b[0m sample_script \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(cleaned_screenplays\u001b[38;5;241m.\u001b[39mkeys())[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'screenplays' is not defined"
     ]
    }
   ],
   "source": [
    "# Function to clean and preprocess text\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Cleans and tokenizes text by:\n",
    "    - Converting to lowercase to standardize text\n",
    "    - Removing punctuation to reduce noise\n",
    "    - Tokenizing words using nltk's word_tokenize function\n",
    "    - Removing stopwords to focus on meaningful words\n",
    "    \n",
    "    This ensures that only relevant words are retained for analysis, improving the accuracy of text-based models.\n",
    "    \n",
    "    Returns:\n",
    "        str: The cleaned and processed text as a single string\n",
    "    \"\"\"\n",
    "    text = text.lower()  # Convert text to lowercase\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation using regex\n",
    "    tokens = word_tokenize(text)  # Tokenize text into words\n",
    "    tokens = [word for word in tokens if word not in stopwords.words('english')]  # Remove stopwords\n",
    "    return ' '.join(tokens)  # Join tokens back into a cleaned string\n",
    "\n",
    "# Apply text preprocessing to all screenplays\n",
    "cleaned_screenplays = {title: preprocess_text(text) for title, text in screenplays.items()}\n",
    "\n",
    "# Display a preview of cleaned text\n",
    "sample_script = list(cleaned_screenplays.keys())[0]\n",
    "print(f\"\\nCleaned Preview of {sample_script}:\\n\")\n",
    "print(cleaned_screenplays[sample_script][:500])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a78b028-3b41-4616-a82a-2dd5fb862fb3",
   "metadata": {},
   "source": [
    "Cell 4: Create the Document-Term Matrix (DTM)\n",
    "This cell transforms the cleaned text into a Document-Term Matrix (DTM) using CountVectorizer. The DTM represents the frequency of each word in the documents (screenplays). Each row corresponds to a screenplay, and each column corresponds to a unique word found in the entire corpus.\n",
    "\n",
    "We then create a pandas DataFrame to make the matrix more readable, with words as columns and screenplays as rows. Finally, we display the first few rows of the DTM to verify its structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b2838eb-51e1-4a10-9c80-59371c2c46f6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cleaned_screenplays' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Convert cleaned text into a Document-Term Matrix (DTM)\u001b[39;00m\n\u001b[1;32m      2\u001b[0m vectorizer \u001b[38;5;241m=\u001b[39m CountVectorizer()\n\u001b[0;32m----> 3\u001b[0m dtm \u001b[38;5;241m=\u001b[39m vectorizer\u001b[38;5;241m.\u001b[39mfit_transform(\u001b[43mcleaned_screenplays\u001b[49m\u001b[38;5;241m.\u001b[39mvalues())\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Create a DataFrame for easier visualization\u001b[39;00m\n\u001b[1;32m      6\u001b[0m dtm_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(dtm\u001b[38;5;241m.\u001b[39mtoarray(), index\u001b[38;5;241m=\u001b[39mcleaned_screenplays\u001b[38;5;241m.\u001b[39mkeys(), columns\u001b[38;5;241m=\u001b[39mvectorizer\u001b[38;5;241m.\u001b[39mget_feature_names_out())\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cleaned_screenplays' is not defined"
     ]
    }
   ],
   "source": [
    "# Convert cleaned text into a Document-Term Matrix (DTM)\n",
    "vectorizer = CountVectorizer()\n",
    "dtm = vectorizer.fit_transform(cleaned_screenplays.values())\n",
    "\n",
    "# Create a DataFrame for easier visualization\n",
    "dtm_df = pd.DataFrame(dtm.toarray(), index=cleaned_screenplays.keys(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "# Display the first few rows of the DTM to examine word frequency across scripts\n",
    "print(dtm_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a51889e4-1ab5-4419-ab11-211a72e301da",
   "metadata": {},
   "source": [
    "Cell 5: Save the Document-Term Matrix as CSV\n",
    "This cell saves the Document-Term Matrix (DTM) into a CSV file, allowing for further analysis and exportation. The CSV file will contain the frequency of each word across the screenplays. Saving it in this format ensures that the DTM can be loaded and analyzed later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3644e49a-e2f1-4eea-9260-d590f5963705",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dtm_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Save the Document-Term Matrix as a CSV file for future analysis\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mdtm_df\u001b[49m\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscreenplays_dtm.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDocument-Term Matrix saved as \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscreenplays_dtm.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dtm_df' is not defined"
     ]
    }
   ],
   "source": [
    "# Save the Document-Term Matrix as a CSV file for future analysis\n",
    "dtm_df.to_csv(\"screenplays_dtm.csv\")\n",
    "print(\"Document-Term Matrix saved as 'screenplays_dtm.csv'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27326acc-cc93-4111-9a47-e77ce1fa14fb",
   "metadata": {},
   "source": [
    " FINAL OUTCOME:\n",
    " This cell summarizes the final outcomes of the script. After processing and generating the Document-Term Matrix (DTM), we now have a structured dataset that can be used for various text analysis tasks:\n",
    "\n",
    "Word frequency analysis to study the most common and unique words in screenplays.\n",
    "\n",
    "Topic modeling to identify themes or genres.\n",
    "\n",
    "Sentiment analysis to examine the emotional tone of the scripts.\n",
    "\n",
    "Machine learning to build models for classifying screenplays automatically.\n",
    "\n",
    "This provides a foundation for any of the above analysis.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c6ed80-c3e0-4e8a-8701-3f6ce5f9d21e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
