{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2db78a3-3296-4a41-86ff-b6ca4bb40d41",
   "metadata": {},
   "source": [
    "Cell 1:Import Libraries and Setup NLTK \n",
    "In this cell, we import the required libraries. We use:\n",
    "\n",
    "os for interacting with the file system (loading screenplays).\n",
    "\n",
    "re for regular expressions, which help clean and preprocess the text.\n",
    "\n",
    "nltk to process natural language, including downloading tokenizers and stopwords.\n",
    "\n",
    "pandas for organizing the data into a DataFrame.\n",
    "\n",
    "CountVectorizer from sklearn to convert text into a Document-Term Matrix (DTM) for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9f55855-7cd7-4a3e-b2f5-6ff01a7f0251",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/beauxcreel/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/beauxcreel/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Setup: Ensure NLTK knows where to download data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fdacc62-6086-4cd4-9e2b-8ac288a403d6",
   "metadata": {},
   "source": [
    "Cell 2:Load Screenplay Files\n",
    "This section defines the folder path containing screenplay files, limits how many to process, and loads the contents into a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8476399d-c39c-4f03-91ef-590f1a8b410c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 5 screenplays.\n",
      "Loaded screenplays: ['aladdin.txt', 'princessbridethe.txt', 'findingnemo.txt', 'kungfupanda.txt', 'e.t..txt']\n"
     ]
    }
   ],
   "source": [
    "folder_path = \"/Users/beauxcreel/code/ENGL370-2025/Creel/Family\"\n",
    "\n",
    "screenplay_files = [f for f in os.listdir(folder_path) if f.endswith('.txt')]\n",
    "MAX_FILES = 5\n",
    "screenplay_files = screenplay_files[:MAX_FILES]\n",
    "\n",
    "screenplays = {}\n",
    "for file in screenplay_files:\n",
    "    with open(os.path.join(folder_path, file), \"r\", encoding=\"utf-8\") as f:\n",
    "        screenplays[file] = f.read()\n",
    "\n",
    "print(f\"Loaded {len(screenplays)} screenplays.\")\n",
    "print(\"Loaded screenplays:\", list(screenplays.keys()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f5056ab-4839-42c7-9965-465fb9dcbb12",
   "metadata": {},
   "source": [
    "Cell 3: preview one screenplay \n",
    "\n",
    "This gives a preview of the first 500 characters from the first screenplay loaded so you can check its content before processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ad9147dd-eb4f-4398-b203-41de02c9fd3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preview of aladdin.txt:\n",
      "\n",
      "ALADDIN:  THE COMPLETE SCRIPT\n",
      "\n",
      "COMPILED BY BEN SCRIPPS \n",
      "\n",
      "(Portions Copyright (c) 1992 The Walt Disney Company\n",
      "\n",
      "PEDDLER:    Oh I come from a land\n",
      "\n",
      "    From a faraway place\n",
      "\n",
      "    Where the caravan camels roam\n",
      "\n",
      "    Where they cut off your ear /Where it's flat and immense\n",
      "\n",
      "    If they don't like your face /And the heat is intense\n",
      "\n",
      "    It's barbaric, but hey--it's home!\n",
      "\n",
      "    When the wind's at your back\n",
      "\n",
      "    And the sun's from the west\n",
      "\n",
      "    And the sand in the glass is right\n",
      "\n",
      "    Come on down,\n",
      "\n",
      "    St\n"
     ]
    }
   ],
   "source": [
    "sample_script = list(screenplays.keys())[0]\n",
    "print(f\"\\nPreview of {sample_script}:\\n\")\n",
    "print(screenplays[sample_script][:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a78b028-3b41-4616-a82a-2dd5fb862fb3",
   "metadata": {},
   "source": [
    "Cell 4: Define preprocessing function\n",
    "This function cleans each script by lowercasing, removing punctuation, tokenizing, and removing stopwords to prepare for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1b2838eb-51e1-4a10-9c80-59371c2c46f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [word for word in tokens if word not in stopwords.words('english')]\n",
    "    return ' '.join(tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a51889e4-1ab5-4419-ab11-211a72e301da",
   "metadata": {},
   "source": [
    "Cell 5: Apply preprocessing to all screenplays\n",
    "This loop applies the preprocessing function to every screenplay, cleaning the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3644e49a-e2f1-4eea-9260-d590f5963705",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cleaned_screenplays = {title: preprocess_text(text) for title, text in screenplays.items()}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e2a5cc-c5bd-49b2-bcea-b7da310a3fee",
   "metadata": {},
   "source": [
    " Cell 6: Preview cleaned text\n",
    "Here we print the first 500 characters of the cleaned version of one screenplay to verify the cleaning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0678978f-98a1-43c8-96d8-580ade665daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_script = list(cleaned_screenplays.keys())[0]\n",
    "print(f\"\\nCleaned Preview of {sample_script}:\\n\")\n",
    "print(cleaned_screenplays[sample_script][:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff72d71b-9604-48be-86a3-20aca34d7ae4",
   "metadata": {},
   "source": [
    "Cell 7: Create Document-Term Matrix (DTM)\n",
    "This cell uses `CountVectorizer` to convert the cleaned text into a numerical format showing the frequency of each word across the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8d014dc2-37a1-423a-8483-981a9b1e172e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "dtm = vectorizer.fit_transform(cleaned_screenplays.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f013e6-daf1-4cd8-bbaa-fb0d5677d678",
   "metadata": {},
   "source": [
    " Cell 8: Convert to DataFrame\n",
    "This transforms the DTM into a Pandas DataFrame so it's easier to view and manipulate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dd8b7c28-103b-46be-9eb8-2c1eeb83a333",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      10  100  101  102  103  104  105  106  106a  107  ...  \\\n",
      "aladdin.txt            0    0    0    0    0    0    0    0     0    0  ...   \n",
      "princessbridethe.txt   1    1    1    1    1    1    1    1     0    1  ...   \n",
      "findingnemo.txt        1    1    0    0    0    0    0    1     1    1  ...   \n",
      "kungfupanda.txt        1    0    0    0    0    0    0    0     0    0  ...   \n",
      "e.t..txt               0    0    0    0    0    0    0    0     0    0  ...   \n",
      "\n",
      "                      zero  zeroed  zings  zip  zips  zombie  zones  zoo  \\\n",
      "aladdin.txt              0       0      1    0     1       1      0    1   \n",
      "princessbridethe.txt     0       1      0    0     0       0      0    0   \n",
      "findingnemo.txt          0       0      0    0     0       0      4    1   \n",
      "kungfupanda.txt          4       0      0    0     0       0      0    0   \n",
      "e.t..txt                 0       0      0    1     2       0      0    0   \n",
      "\n",
      "                      zoom  zooms  \n",
      "aladdin.txt              4      8  \n",
      "princessbridethe.txt     0      1  \n",
      "findingnemo.txt          0      0  \n",
      "kungfupanda.txt          1      0  \n",
      "e.t..txt                 0      0  \n",
      "\n",
      "[5 rows x 7402 columns]\n"
     ]
    }
   ],
   "source": [
    "dtm_df = pd.DataFrame(dtm.toarray(), index=cleaned_screenplays.keys(), columns=vectorizer.get_feature_names_out())\n",
    "print(dtm_df.head()) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a52428-1310-45d5-b83c-96c3de44ba0e",
   "metadata": {},
   "source": [
    " Cell 9: Save DTM to CSV\n",
    "Finally, the structured document-term data is saved to a CSV file so it can be used in later analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a3948785-7331-4502-a366-6ae59391ca24",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document-Term Matrix saved as 'screenplays_dtm.csv'.\n"
     ]
    }
   ],
   "source": [
    "dtm_df.to_csv(\"screenplays_dtm.csv\")\n",
    "print(\"Document-Term Matrix saved as 'screenplays_dtm.csv'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27326acc-cc93-4111-9a47-e77ce1fa14fb",
   "metadata": {},
   "source": [
    " FINAL OUTCOME:\n",
    " This cell summarizes the final outcomes of the script. After processing and generating the Document-Term Matrix (DTM), we now have a structured dataset that can be used for various text analysis tasks:\n",
    "\n",
    "Word frequency analysis to study the most common and unique words in screenplays.\n",
    "\n",
    "Topic modeling to identify themes or genres.\n",
    "\n",
    "Sentiment analysis to examine the emotional tone of the scripts.\n",
    "\n",
    "Machine learning to build models for classifying screenplays automatically.\n",
    "\n",
    "This provides a foundation for any of the above analysis.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c6ed80-c3e0-4e8a-8701-3f6ce5f9d21e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
