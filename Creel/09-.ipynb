{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00da53a8-5774-49cf-84f6-5f608662bf14",
   "metadata": {},
   "source": [
    "Cell 1: Loading the Text Files\n",
    "In this first part, I load in all of the .txt files from my Family folder. I use Python’s pathlib to find all text files in that folder and read their contents. I store the text from each file in a dictionary, using the file name as the key. This lets me keep track of which file each text came from, which could be helpful later if I want to go back and look at how certain topics show up in specific documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f039387-939a-4020-8f5f-301c5066ae7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 39 files from 'Family' folder:\n",
      "- aladdin.txt\n",
      "- princessbridethe.txt\n",
      "- findingnemo.txt\n",
      "- kungfupanda.txt\n",
      "- e.t..txt\n",
      "- shrek.txt\n",
      "- walle.txt\n",
      "- itsawonderfullife.txt\n",
      "- labyrinth.txt\n",
      "- speedracer.txt\n",
      "- marypoppins.txt\n",
      "- riseoftheguardians.txt\n",
      "- big.txt\n",
      "- happyfeet.txt\n",
      "- mulan.txt\n",
      "- pacifierthe.txt\n",
      "- mightymorphinpowerrangersthemovie.txt\n",
      "- fantasticmrfox.txt\n",
      "- wizardofozthe.txt\n",
      "- up.txt\n",
      "- fieldofdreams.txt\n",
      "- transformersthemovie.txt\n",
      "- flintstonesthe.txt\n",
      "- megamind.txt\n",
      "- newsies.txt\n",
      "- anastasia.txt\n",
      "- littlemermaidthe.txt\n",
      "- coraline.txt\n",
      "- threemenandababy.txt\n",
      "- ducksoup.txt\n",
      "- marleyme.txt\n",
      "- rescuersdownunderthe.txt\n",
      "- newyorkminute.txt\n",
      "- chroniclesofnarniathelionthewitchandthewardrobe.txt\n",
      "- addamsfamilythe.txt\n",
      "- mygirl.txt\n",
      "- sandlotkidsthe.txt\n",
      "- pokemonmewtworeturns.txt\n",
      "- toystory.txt\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Set folder to \"Family\"\n",
    "family_folder = Path(\"Family\")\n",
    "\n",
    "# Get all .txt files in the \"Family\" folder\n",
    "family_files = list(family_folder.glob(\"*.txt\"))\n",
    "\n",
    "# Load contents of all text files into a dictionary\n",
    "family_texts = {}\n",
    "for file_path in family_files:\n",
    "    with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "        family_texts[file_path.name] = f.read()\n",
    "\n",
    "# Summary output\n",
    "print(f\"Loaded {len(family_texts)} files from 'Family' folder:\")\n",
    "for name in family_texts:\n",
    "    print(\"-\", name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68746d7e-7b61-47bc-ae5e-0ab9ea6325a9",
   "metadata": {},
   "source": [
    "Cell 2: Tokenizing and Removing Proper Nouns\n",
    "Here I start cleaning the texts. First, I tokenize the words (break the text into individual words) using NLTK. Then I tag each word with its part of speech so I can filter out proper nouns (like names and places). These tend to be very specific and can distract from the broader themes I'm trying to find. After removing them, I join the words back together so they’re ready for analysis. This helps make sure the topic model doesn’t get biased by specific character names or locations that show up a lot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72c5e6c1-6280-467d-8d17-60403aae9fd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/beauxcreel/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/beauxcreel/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text preprocessing complete. Ready for vectorization.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "\n",
    "# Download NLTK models (only once)\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# Step 1: Tokenize each document\n",
    "family_tokenized = {\n",
    "    filename: word_tokenize(text)\n",
    "    for filename, text in family_texts.items()\n",
    "}\n",
    "\n",
    "# Step 2: Remove proper nouns (NNP, NNPS)\n",
    "family_cleaned = {\n",
    "    filename: [\n",
    "        word for word, tag in pos_tag(tokens)\n",
    "        if tag not in ('NNP', 'NNPS')\n",
    "    ]\n",
    "    for filename, tokens in family_tokenized.items()\n",
    "}\n",
    "\n",
    "# Step 3: Rejoin cleaned tokens into strings\n",
    "family_joined_cleaned = {\n",
    "    filename: ' '.join(words)\n",
    "    for filename, words in family_cleaned.items()\n",
    "}\n",
    "\n",
    "print(\"Text preprocessing complete. Ready for vectorization.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad3009ce-ee35-43e4-80ab-67dd5b55a10a",
   "metadata": {},
   "source": [
    "Cell 3: TF-IDF Vectorization\n",
    "This cell turns my cleaned texts into numbers that the computer can understand using something called TF-IDF. It looks at how important each word is in a document, compared to how often it shows up across all documents. I also set two filters:\n",
    "\n",
    "min_df=5: this skips words that only show up in a few texts (too rare).\n",
    "\n",
    "max_df=0.9: this skips words that show up in almost every text (too common).\n",
    "\n",
    "These settings help me focus on the words that are most useful for finding topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6148aa0-fec7-4687-b2cb-4a0c2de74374",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF matrix shape: (39, 4810)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Set TF-IDF parameters (can experiment with min_df and max_df later)\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    max_df=0.9,\n",
    "    min_df=5,\n",
    "    stop_words='english'\n",
    ")\n",
    "\n",
    "# Apply TF-IDF vectorization\n",
    "family_dtm = tfidf_vectorizer.fit_transform(family_joined_cleaned.values())\n",
    "\n",
    "print(\"TF-IDF matrix shape:\", family_dtm.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b83efa6-eb1f-4c82-8a5c-d94cd32874b9",
   "metadata": {},
   "source": [
    "Cell 4: Topic Modeling with NMF\n",
    "Now I actually build the topic model using NMF (Non-negative Matrix Factorization). I start by asking it to find 10 topics. The model goes through the TF-IDF matrix and tries to group words that appear together often across documents. Then, for each topic, I look at the top 10 words to get a sense of what that topic is about. This gives me a list of possible themes across the texts — like family relationships, childhood, home life, etc. I chose NMF because it usually gives cleaner, more interpretable topics than LDA when working with TF-IDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dfdd6f2c-6256-4013-a00c-c6a6142c7c1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "NMF TOPIC #1:\n",
      " ['finally', 'north', 'pitch', 'staff', 'scroll', '12', 'power', 'master', 'kids', 'suddenly']\n",
      "\n",
      "NMF TOPIC #2:\n",
      " ['transforms', 'reacts', 'interview', 'ice', 'forward', 'dog', 'speaks', 'speed', 'car', 'continued']\n",
      "\n",
      "NMF TOPIC #3:\n",
      " ['human', 'kiss', 'ocean', 'ta', 'gasps', 'swim', 'swimming', 'sea', 'ha', 'fish']\n",
      "\n",
      "NMF TOPIC #4:\n",
      " ['table', 'window', 'living', 'scene', 'office', 'money', 'baby', 'house', 'phone', 'car']\n",
      "\n",
      "NMF TOPIC #5:\n",
      " ['play', 'scene', 'gang', 'field', 'fence', 'glove', 'squints', 'ball', 'baseball', 'yeah']\n",
      "\n",
      "NMF TOPIC #6:\n",
      " ['window', 'boys', 'papers', 'ai', 'continued', '11', 'family', '165', '90', '91']\n",
      "\n",
      "NMF TOPIC #7:\n",
      " ['console', 'audience', 'wild', 'passengers', 'suddenly', 'ship', 'ext', 'plant', 'int', 'wall']\n",
      "\n",
      "NMF TOPIC #8:\n",
      " ['ground', 'arrow', 'palace', 'castle', 'troops', 'walks', 'tent', 'princess', 'dragon', 'sword']\n",
      "\n",
      "NMF TOPIC #9:\n",
      " ['father', 'sugar', 'step', 'bank', 'boom', 'banks', 'medicine', 'sir', 'children', 'mary']\n",
      "\n",
      "NMF TOPIC #10:\n",
      " ['audience', 'window', 'walks', 'truck', 'cat', 'doll', 'dogs', 'toys', 'bird', 'house']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/370/lib/python3.13/site-packages/sklearn/decomposition/_nmf.py:1742: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "\n",
    "# Create and fit NMF model (10 topics to start)\n",
    "nmf_model = NMF(n_components=10, random_state=42)\n",
    "family_nmf_topics = nmf_model.fit_transform(family_dtm)\n",
    "\n",
    "# Get feature (word) names from vectorizer\n",
    "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "# Display top 10 words per topic\n",
    "for topic_idx, topic in enumerate(nmf_model.components_):\n",
    "    top_words = [feature_names[i] for i in topic.argsort()[-10:]]\n",
    "    print(f\"\\nNMF TOPIC #{topic_idx + 1}:\\n\", top_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb819753-cbcb-4139-ba88-35207a8ce6d6",
   "metadata": {},
   "source": [
    "Cell 5: Optional — Try LDA for Comparison\n",
    "In this extra step, I use LDA (Latent Dirichlet Allocation), another popular topic modeling method. It works a bit differently than NMF and is based on probabilities. I use the same number of topics (10) and the same data to see if the results are clearer or more interesting. This gives me a way to compare and decide which model works best for this particular text collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e52e98af-59bf-4e8d-b197-2a3121f569fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LDA TOPIC #1:\n",
      " ['west', 'exaggerate', 'wobbly', 'delicate', 'east', 'penny', 'responsible', 'riddance', 'loops', 'replace']\n",
      "\n",
      "LDA TOPIC #2:\n",
      " ['sits', 'begins', 'wall', 'window', 'walks', 'house', 'yeah', 'car', 'suddenly', 'continued']\n",
      "\n",
      "LDA TOPIC #3:\n",
      " ['west', 'exaggerate', 'wobbly', 'delicate', 'east', 'penny', 'responsible', 'riddance', 'loops', 'replace']\n",
      "\n",
      "LDA TOPIC #4:\n",
      " ['west', 'exaggerate', 'wobbly', 'delicate', 'east', 'penny', 'responsible', 'riddance', 'loops', 'replace']\n",
      "\n",
      "LDA TOPIC #5:\n",
      " ['west', 'exaggerate', 'wobbly', 'delicate', 'east', 'penny', 'responsible', 'riddance', 'loops', 'replace']\n",
      "\n",
      "LDA TOPIC #6:\n",
      " ['west', 'exaggerate', 'wobbly', 'delicate', 'east', 'penny', 'responsible', 'riddance', 'loops', 'replace']\n",
      "\n",
      "LDA TOPIC #7:\n",
      " ['west', 'exaggerate', 'wobbly', 'delicate', 'east', 'penny', 'responsible', 'riddance', 'loops', 'replace']\n",
      "\n",
      "LDA TOPIC #8:\n",
      " ['west', 'exaggerate', 'wobbly', 'delicate', 'east', 'penny', 'responsible', 'riddance', 'loops', 'replace']\n",
      "\n",
      "LDA TOPIC #9:\n",
      " ['west', 'exaggerate', 'wobbly', 'delicate', 'east', 'penny', 'responsible', 'riddance', 'loops', 'replace']\n",
      "\n",
      "LDA TOPIC #10:\n",
      " ['west', 'exaggerate', 'wobbly', 'delicate', 'east', 'penny', 'responsible', 'riddance', 'loops', 'replace']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation as LDA\n",
    "\n",
    "# Fit an LDA model using the same DTM\n",
    "lda_model = LDA(n_components=10, random_state=42)\n",
    "family_lda_topics = lda_model.fit_transform(family_dtm)\n",
    "\n",
    "# Display top 10 words per LDA topic\n",
    "for topic_idx, topic in enumerate(lda_model.components_):\n",
    "    top_words = [feature_names[i] for i in topic.argsort()[-10:]]\n",
    "    print(f\"\\nLDA TOPIC #{topic_idx + 1}:\\n\", top_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47009189-6ff3-4c81-9571-98f585e0f885",
   "metadata": {},
   "source": [
    "Final Thoughts\n",
    "Overall, this notebook sets up a full pipeline from raw text to topic analysis. The results depend a lot on the settings I use — like how many topics I choose and which words I include or leave out. In future steps, I plan to test different numbers of topics and try changing the min_df and max_df values to see how it affects the topics I get."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
