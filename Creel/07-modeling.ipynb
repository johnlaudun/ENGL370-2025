{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f030b2-4281-473b-b5d9-d276838654c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460b7b0f-e1b9-40b8-abb8-7c14c10d0b28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "906d5ebc-87e8-48ad-9749-7e182684ecaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 0 documents.\n",
      "\n",
      "Preview of first 3 documents:\n",
      "[]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "empty vocabulary; perhaps the documents only contain stop words",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 29\u001b[0m\n\u001b[1;32m     26\u001b[0m vectorizer_lda \u001b[38;5;241m=\u001b[39m CountVectorizer(stop_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m'\u001b[39m, min_df\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, max_df\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.85\u001b[39m)\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Fit the vectorizer and transform the documents into a Document-Term Matrix (DTM)\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m dtm_lda \u001b[38;5;241m=\u001b[39m \u001b[43mvectorizer_lda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# Fit the LDA model\u001b[39;00m\n\u001b[1;32m     32\u001b[0m lda \u001b[38;5;241m=\u001b[39m LatentDirichletAllocation(n_components\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)  \u001b[38;5;66;03m# Change n_components based on your needs\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/370/lib/python3.13/site-packages/sklearn/base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1387\u001b[0m     )\n\u001b[1;32m   1388\u001b[0m ):\n\u001b[0;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/370/lib/python3.13/site-packages/sklearn/feature_extraction/text.py:1376\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1368\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1369\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpper case characters found in\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1370\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m vocabulary while \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlowercase\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1371\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is True. These entries will not\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1372\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m be matched with any documents\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1373\u001b[0m             )\n\u001b[1;32m   1374\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m-> 1376\u001b[0m vocabulary, X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_count_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfixed_vocabulary_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1378\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbinary:\n\u001b[1;32m   1379\u001b[0m     X\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfill(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/370/lib/python3.13/site-packages/sklearn/feature_extraction/text.py:1282\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m   1280\u001b[0m     vocabulary \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(vocabulary)\n\u001b[1;32m   1281\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m vocabulary:\n\u001b[0;32m-> 1282\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1283\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mempty vocabulary; perhaps the documents only contain stop words\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1284\u001b[0m         )\n\u001b[1;32m   1286\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m indptr[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39miinfo(np\u001b[38;5;241m.\u001b[39mint32)\u001b[38;5;241m.\u001b[39mmax:  \u001b[38;5;66;03m# = 2**31 - 1\u001b[39;00m\n\u001b[1;32m   1287\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _IS_32BIT:\n",
      "\u001b[0;31mValueError\u001b[0m: empty vocabulary; perhaps the documents only contain stop words"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import glob\n",
    "from sklearn.decomposition import LatentDirichletAllocation, NMF\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# Path to the corpus directory\n",
    "path_to_corpus = r'C:\\Users\\beaux creel\\code\\ENGL370-2025\\ENGL370-2025\\Creel\\Family.'\n",
    "\n",
    "# Read all documents from the corpus directory\n",
    "documents = [open(file, 'r', encoding='utf-8').read() for file in glob.glob(os.path.join(path_to_corpus, '*.txt'))]\n",
    "\n",
    "# Check the first few documents to ensure they're loaded properly\n",
    "print(f\"Loaded {len(documents)} documents.\\n\")\n",
    "print(\"Preview of first 3 documents:\")\n",
    "print(documents[:3])\n",
    "\n",
    "# Clean the documents (strip unnecessary whitespace, remove any empty docs)\n",
    "documents = [doc.strip() for doc in documents if doc.strip()]\n",
    "\n",
    "# -------------------------\n",
    "# Topic Modeling with LDA\n",
    "# -------------------------\n",
    "\n",
    "# Create the CountVectorizer for LDA (use stop word removal)\n",
    "vectorizer_lda = CountVectorizer(stop_words='english', min_df=2, max_df=0.85)\n",
    "\n",
    "# Fit the vectorizer and transform the documents into a Document-Term Matrix (DTM)\n",
    "dtm_lda = vectorizer_lda.fit_transform(documents)\n",
    "\n",
    "# Fit the LDA model\n",
    "lda = LatentDirichletAllocation(n_components=4, random_state=42)  # Change n_components based on your needs\n",
    "lda.fit(dtm_lda)\n",
    "\n",
    "# Display the top 5 words for each topic from the LDA model\n",
    "print(\"\\nLDA Topics:\")\n",
    "for topic_idx, topic in enumerate(lda.components_):\n",
    "    print(f\"Topic {topic_idx + 1}:\")\n",
    "    top_words = [vectorizer_lda.get_feature_names_out()[i] for i in topic.argsort()[:-6 - 1:-1]]  # Top 5 words\n",
    "    print(\" \".join(top_words))\n",
    "    print(\"\\n\")\n",
    "\n",
    "# -------------------------\n",
    "# Topic Modeling with NMF\n",
    "# -------------------------\n",
    "\n",
    "# Create the TfidfVectorizer for NMF (also with stop words removed)\n",
    "vectorizer_nmf = TfidfVectorizer(stop_words='english', min_df=2, max_df=0.85)\n",
    "\n",
    "# Fit the vectorizer and transform the documents into a Document-Term Matrix (DTM)\n",
    "dtm_nmf = vectorizer_nmf.fit_transform(documents)\n",
    "\n",
    "# Fit the NMF model\n",
    "nmf = NMF(n_components=4, random_state=42)  # Change n_components based on your needs\n",
    "nmf.fit(dtm_nmf)\n",
    "\n",
    "# Display the top 5 words for each topic from the NMF model\n",
    "print(\"\\nNMF Topics:\")\n",
    "for topic_idx, topic in enumerate(nmf.components_):\n",
    "    print(f\"Topic {topic_idx + 1}:\")\n",
    "    top_words = [vectorizer_nmf.get_feature_names_out()[i] for i in topic.argsort()[:-6 - 1:-1]]  # Top 5 words\n",
    "    print(\" \".join(top_words))\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed907836-38e1-46b6-8dcd-5c6552cb6271",
   "metadata": {},
   "outputs": [],
   "source": [
    "ran the code but it wasnt reading any of my documents or even registering them as vocab. File /opt/anaconda3/envs/370/lib/python3.13/site-packages/sklearn/base.py:1389, in _fit_context.<locals>.decorator.<locals>.wrapper(estimator, *args, **kwargs)\n",
    "   1382     estimator._validate_params()\n",
    "   1384 with config_context(\n",
    "   1385     skip_parameter_validation=(\n",
    "   1386         prefer_skip_nested_validation or global_skip_validation\n",
    "   1387     )\n",
    "   1388 ):\n",
    "-> 1389     return fit_method(estimator, *args, **kwargs)\n",
    "\n",
    "File /opt/anaconda3/envs/370/lib/python3.13/site-packages/sklearn/feature_extraction/text.py:1376, in CountVectorizer.fit_transform(self, raw_documents, y)\n",
    "   1368             warnings.warn(\n",
    "   1369                 \"Upper case characters found in\"\n",
    "   1370                 \" vocabulary while 'lowercase'\"\n",
    "   1371                 \" is True. These entries will not\"\n",
    "   1372                 \" be matched with any documents\"\n",
    "   1373             )\n",
    "   1374             break\n",
    "-> 1376 vocabulary, X = self._count_vocab(raw_documents, self.fixed_vocabulary_)\n",
    "   1378 if self.binary:\n",
    "   1379     X.data.fill(1)\n",
    "\n",
    "File /opt/anaconda3/envs/370/lib/python3.13/site-packages/sklearn/feature_extraction/text.py:1282, in CountVectorizer._count_vocab(self, raw_documents, fixed_vocab)\n",
    "   1280     vocabulary = dict(vocabulary)\n",
    "   1281     if not vocabulary:\n",
    "-> 1282         raise ValueError(\n",
    "   1283             \"empty vocabulary; perhaps the documents only contain stop words\"\n",
    "   1284         )\n",
    "   1286 if indptr[-1] > np.iinfo(np.int32).max:  # = 2**31 - 1\n",
    "   1287     if _IS_32BIT:\n",
    "\n",
    "ValueError: empty vocabulary; perhaps the documents only contain stop words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67930839-d25f-42d5-bbd1-5c8d362b385b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
